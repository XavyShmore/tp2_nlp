{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d53d4c2",
      "metadata": {
        "id": "4d53d4c2"
      },
      "source": [
        "# Tâche 2 : Question-réponse avec modèle pré‑entraîné GPT‑2, sans adaptation\n",
        "\n",
        "**Objectifs**\n",
        "\n",
        "Observer les limites d’un modèle de langage **pré‑entraîné** (**GPT-2 Medium (355M)**) lorsqu’on lui pose des questions sur un sujet **absent** de ses données d’origine (ici, l’univers de *Sherlock Holmes*).\n",
        "Dans ce *notebook*, vous faites **uniquement de l’inférence** avec un modèle qui a déjà été pré-entraîné. Cette tâche consiste à construire un *prompt* minimal, générer des réponses avec le modèle sans modification, puis évaluer la pertinence des résultats. Aucun entraînement de modèle n'est effectué pour cette tâche.\n",
        "\n",
        "**Objectifs d’apprentissage**\n",
        "1. Charger un modèle pré‑entraîné et son tokenizer de Hugging Face.\n",
        "2. Générer du texte et **isoler la réponse** du modèle.\n",
        "3. Comprendre et expliquer les **limites du pré‑entraînement** hors‑domaine.\n",
        "\n",
        "Les **questions** pour évaluer le modèle vous sont fournies. Vous devez comprendre le format des questions chargées en mémoire.\n",
        "\n",
        "> Il est recommandé de faire ce travail pratique en utilisant une carte graphique GPU compatible avec HuggingFace/Pytorch.\n",
        "> Si votre machine n’en possède pas, vous pouvez utiliser **Google Colab** pour exécuter le *notebook* dans le cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81af0ea3655c4578",
      "metadata": {
        "id": "81af0ea3655c4578"
      },
      "source": [
        "Si nécessaire, installer les *packages* suivant. Si vous exécutez sur Code Colab, ces *packages* devraient déjà être installés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P1jvDsOV7WVW",
      "metadata": {
        "id": "P1jvDsOV7WVW"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install accelerate\n",
        "#!pip install transformers[torch]\n",
        "#!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "660a299e09c87174",
      "metadata": {
        "id": "660a299e09c87174"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "dossier_cible = '/content/drive/MyDrive/uni/nlp/nlp_tp2_models'\n",
        "raccourci = '/content/nlp_tp2_models'\n",
        "\n",
        "#Mount a google drive folder to save models\n",
        "if IN_COLAB:\n",
        "  drive.mount('/content/drive')\n",
        "  os.symlink(dossier_cible, raccourci)\n",
        "\n",
        "repo_url = \"https://github.com/XavyShmore/tp2_nlp.git\"\n",
        "if IN_COLAB:\n",
        "  !git clone {repo_url}\n",
        "  !cp -r ./tp2_nlp/data ."
      ],
      "metadata": {
        "id": "wO-pIFizeEJF",
        "outputId": "3463fd2b-ba87-4019-b160-c7e4626353d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wO-pIFizeEJF",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'tp2_nlp'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 19 (delta 7), reused 11 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (19/19), 1.46 MiB | 19.18 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4d460d1d84efc873",
      "metadata": {
        "id": "4d460d1d84efc873"
      },
      "outputs": [],
      "source": [
        "batch_size = 5 # il est possible d'ajuster la taille de batch. Les valeurs actuelles utilisent environ 10 Gb\n",
        "max_length = 256 # on réduit le contexte pour sauver du temps, nos exemples ne nécesside pas une plus grande fenêtre de mots\n",
        "model_name = \"gpt2-medium\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc5be8b4054923c",
      "metadata": {
        "id": "ffc5be8b4054923c"
      },
      "source": [
        "## 1. Chargement du modèle Hugging Face et du tokenizer (à compléter)\n",
        "\n",
        "Complétez le corps de la fonction `load_model(model_path)` afin qu’elle :\n",
        "\n",
        "- charge le **tokenizer** et le **modèle** Hugging Face à partir du chemin `model_path`.\n",
        "- **retourne** le tokenizer comme **première valeur de retour** et le modèle comme **seconde valeur de retour**.\n",
        "\n",
        "On ajoute également des fonctions pour monter les questions en mémoire et pour sauvegarder les réponses dans un fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "839fc2cb34773ade",
      "metadata": {
        "id": "839fc2cb34773ade"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c56a1e65ebeb313",
      "metadata": {
        "id": "4c56a1e65ebeb313"
      },
      "outputs": [],
      "source": [
        "def load_entries(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(f\"Question file must contain a list of objects. Got: {type(data)}\")\n",
        "    return data\n",
        "\n",
        "def save_answers(questions_answers, output_dir, out_file_name, display=True):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(os.path.join(output_dir, out_file_name), \"w\", encoding=\"utf-8\") as out:\n",
        "        for index, question, answer, expected_answer in questions_answers:\n",
        "            out.write(f\"Q: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\\n\")\n",
        "            if display:\n",
        "                print(f\"Q{index}: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2300e4992db278ff",
      "metadata": {
        "id": "2300e4992db278ff"
      },
      "source": [
        "## 2. Fonctions de test question-réponse  (à compléter)\n",
        "\n",
        "La fonction **test_on_questions** est utilisée pour parcourir **toutes les entrées** du fichier de questions afin de produire des réponses générées par le modèle.\n",
        "\n",
        "La génération d'une réponse à une question implique les étapes suivantes (fonction **process_entry** à compléter) :\n",
        "* Construire un prompt à l’aide de la fonction **build_prompt** (rendu disponible).\n",
        "* Utiliser le modèle (via un pipeline de génération passé en argument) pour générer une réponse à une question.\n",
        "* Retourner la réponse générée par le modèle.  \n",
        "\n",
        "Points importants à souligner:\n",
        "* La fonction ***process_entry*** doit retourner uniquement la réponse générée par le modèle (sans la question - le prompt).\n",
        "* Il est de votre responsabililté de choisir **les paramètres** du générateur (max_new_tokens, do_sample, temperature, top_k ou top_p). Décrivez ceux que vous avez retenus.\n",
        "\n",
        "> Afin de simplifier le travail, nous avons choisi de ne pas utiliser de *batchs* dans la fonction qui teste les questions.\n",
        "> Vous n'avez pas à prendre en compte le *warning* qui suggère d'utiliser des *datasets*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b01c1a71",
      "metadata": {
        "id": "b01c1a71"
      },
      "outputs": [],
      "source": [
        "def build_prompt(entry):\n",
        "    question = entry.get(\"question\", \"\")\n",
        "    prompt = f\"Please answer the following question about the Sherlock Holmes Universe with a short answer.\\nQuestion: {question}\\nAnswer: \"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entries = load_entries(\"data/questions_sherlock.json\")\n",
        "print(build_prompt(entries[0]))"
      ],
      "metadata": {
        "id": "_p2m0GGFmMOG",
        "outputId": "1a597a21-49a7-492e-aad7-3b4bc613daf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_p2m0GGFmMOG",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please answer the following question about the Sherlock Holmes Universe.\n",
            "Question: Where do Sherlock Holmes and Dr. Watson live?\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef5e3424",
      "metadata": {
        "id": "ef5e3424"
      },
      "source": [
        "Description des paramètres de génération:\n",
        "(à compléter...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ddcab8bb4bf71997",
      "metadata": {
        "id": "ddcab8bb4bf71997"
      },
      "outputs": [],
      "source": [
        "def process_entry(entry, prompt_builder, generator):\n",
        "    prompt = prompt_builder(entry)\n",
        "    generation_output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=10, # limit the length of the generated answer\n",
        "        do_sample=False,    # enable sampling for more varied responses\n",
        "        temperature=0.8,   # control the randomness of the output\n",
        "        top_k=50,          # consider only the top k most likely next tokens\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    # The generated_text includes the prompt, so we need to extract only the answer\n",
        "    generated_text = generation_output[0]['generated_text']\n",
        "    answer = generated_text[len(prompt):].strip()\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a7a85afb324813ff",
      "metadata": {
        "id": "a7a85afb324813ff"
      },
      "outputs": [],
      "source": [
        "def test_on_questions(prompt_builder, model_path, question_file, out_file_name, output_dir=\"results\"):\n",
        "    entries = load_entries(question_file)\n",
        "    tokenizer, model = load_model(model_path)\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    results = []\n",
        "    for i, entry in enumerate(entries):\n",
        "        answer = process_entry(entry, prompt_builder, generator)\n",
        "        question = entry.get(\"question\", \"\")\n",
        "        expected_answer = entry.get(\"answer\", \"\")\n",
        "        results.append((i, question, answer, expected_answer))\n",
        "    save_answers(results, output_dir, out_file_name, display=True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19259c4ffd3291b2",
      "metadata": {
        "id": "19259c4ffd3291b2"
      },
      "source": [
        "## 3. Génération avec GPT-2 de réponses aux questions sur Sherlock Holmes\n",
        "\n",
        "Exécutez la cellule suivante pour générer les réponses aux questions.\n",
        "Le temps d’exécution devrait se situer entre **5 et 10 minutes** si vous utilisez **Google Colab** avec un GPU.\n",
        "\n",
        "Note : N'oubliez pas d'ajouter le fichier de réponses générées par le modèle (voir *out_file_name*) dans votre remise du travail. Ainsi le fichier ZIP que vous déposerez sur le site du cours devra contenir tous les *notebooks* et tous les fichiers de réponses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4f6e7995f5a08f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f6e7995f5a08f31",
        "outputId": "954103dd-867a-4058-95ca-d1584c58835c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q0: Where do Sherlock Holmes and Dr. Watson live?\n",
            "A: Sherlock Holmes lives in London, England.\n",
            "Expected:221B Baker Street, London.\n",
            "------------------------------------------------------------\n",
            "Q1: Who is Sherlock Holmes' loyal friend and chronicler?\n",
            "A: The answer is Sherlock Holmes' loyal friend and\n",
            "Expected:Dr. John H. Watson.\n",
            "------------------------------------------------------------\n",
            "Q2: Who is considered 'The Woman' by Sherlock Holmes?\n",
            "A: The Woman is the woman who is the most\n",
            "Expected:Irene Adler.\n",
            "------------------------------------------------------------\n",
            "Q3: Which story features the Red-Headed League?\n",
            "A: The Red-Headed League.\n",
            "Question\n",
            "Expected:The Adventure of the Red-Headed League.\n",
            "------------------------------------------------------------\n",
            "Q4: What is the primary occupation of Sherlock Holmes?\n",
            "A: Sherlock Holmes is a detective.\n",
            "Question\n",
            "Expected:Consulting detective.\n",
            "------------------------------------------------------------\n",
            "Q5: Who is Sherlock Holmes' arch-nemesis?\n",
            "A: The man who killed his wife and child.\n",
            "Expected:Professor James Moriarty.\n",
            "------------------------------------------------------------\n",
            "Q6: What musical instrument does Sherlock Holmes play?\n",
            "A: A violin.\n",
            "Question: What is the\n",
            "Expected:The violin.\n",
            "------------------------------------------------------------\n",
            "Q7: What is the name of Sherlock Holmes' elder brother?\n",
            "A: \"The Great Detective\"\n",
            "Question: What\n",
            "Expected:Mycroft Holmes.\n",
            "------------------------------------------------------------\n",
            "Q8: What is the blue gemstone found inside a Christmas goose called?\n",
            "A: The blue gemstone is a mineral that is\n",
            "Expected:The Blue Carbuncle.\n",
            "------------------------------------------------------------\n",
            "Q9: What residue does Holmes often analyze to identify smokers?\n",
            "A: The residue of the cigarette is the most common\n",
            "Expected:Tobacco ash.\n",
            "------------------------------------------------------------\n",
            "Q10: What tactic besides observation does Holmes often use to gather information?\n",
            "A: He uses observation.  He uses observation\n",
            "Expected:Disguise.\n",
            "------------------------------------------------------------\n",
            "Q11: What was the profession of Dr. John Watson?\n",
            "A: Dr. Watson was a doctor who was a\n",
            "Expected:Doctor.\n",
            "------------------------------------------------------------\n",
            "Q12: What kind of marks on the ground does Holmes often study to track people?\n",
            "A: The marks on the ground are the marks of\n",
            "Expected:Footprints.\n",
            "------------------------------------------------------------\n",
            "Q13: Who is the landlady of 221B Baker Street?\n",
            "A: Sherlock Holmes.\n",
            "Question: Who is\n",
            "Expected:Mrs. Hudson.\n",
            "------------------------------------------------------------\n",
            "Q14: What substance did Sherlock Holmes sometimes use to stimulate his mind?\n",
            "A: A substance that is not alcohol, but is\n",
            "Expected:Cocaine.\n",
            "------------------------------------------------------------\n",
            "Q15: Who is the Scotland Yard detective that often consults Holmes?\n",
            "A: Sherlock Holmes.\n",
            "Question: What is\n",
            "Expected:Inspector Lestrade (Gregson or Bradstreet also acceptable).\n",
            "------------------------------------------------------------\n",
            "Q16: What warning arrives as envelopes containing dried orange seeds?\n",
            "A: A small orange envelope containing dried orange seeds.\n",
            "Expected:Five orange pips.\n",
            "------------------------------------------------------------\n",
            "Q17: What is the bog near Baskerville Hall called?\n",
            "A: Baskerville Hall\n",
            "Question: What is\n",
            "Expected:The Grimpen Mire.\n",
            "------------------------------------------------------------\n",
            "Q18: Which London newspaper does Holmes frequently read?\n",
            "A: The Daily Express.\n",
            "Question: Which London\n",
            "Expected:The Times.\n",
            "------------------------------------------------------------\n",
            "Q19: What kind of drawings form the code that Holmes deciphers on walls and paper?\n",
            "A: The code is a series of symbols that Holmes\n",
            "Expected:Dancing stick figures.\n",
            "------------------------------------------------------------\n",
            "Q20: What kind of jewel is set in the damaged coronet handled by a banker?\n",
            "A: A diamond.\n",
            "Question: What is the\n",
            "Expected:Beryls.\n",
            "------------------------------------------------------------\n",
            "Q21: What combat sport is Holmes proficient in?\n",
            "A: Martial arts.\n",
            "Question: What is\n",
            "Expected:Boxing.\n",
            "------------------------------------------------------------\n",
            "Q22: What does Sherlock Holmes keep in his Persian slipper?\n",
            "A: A small bag of gold coins.\n",
            "Question\n",
            "Expected:Pipe tobacco.\n",
            "------------------------------------------------------------\n",
            "Q23: In which room at 221B do most client interviews happen?\n",
            "A: The Sherlock Holmes Universe is a fictional universe created\n",
            "Expected:The sitting-room.\n",
            "------------------------------------------------------------\n",
            "Q24: What important document goes missing from the Foreign Office in one case?\n",
            "A: The Foreign Office's \"The Case Files of\n",
            "Expected:A secret naval treaty.\n",
            "------------------------------------------------------------\n",
            "Q25: What hot drink is often served at Baker Street?\n",
            "A: A hot drink is usually served at Baker Street\n",
            "Expected:Tea.\n",
            "------------------------------------------------------------\n",
            "Q26: What object ties Irene Adler to a scandal with a European king?\n",
            "A: The Sherlock Holmes Universe is a fictional universe created\n",
            "Expected:A compromising photograph.\n",
            "------------------------------------------------------------\n",
            "Q27: What weapon does Dr. Watson often carry?\n",
            "A: A knife.\n",
            "Question: What is the\n",
            "Expected:A revolver.\n",
            "------------------------------------------------------------\n",
            "Q28: In which country did Dr. Watson serve as an army doctor?\n",
            "A: England\n",
            "Question: In which country did Dr\n",
            "Expected:Afghanistan.\n",
            "------------------------------------------------------------\n",
            "Q29: What is the missing racehorse’s name in the Dartmoor case?\n",
            "A: The missing racehorse's name is\n",
            "Expected:Silver Blaze.\n",
            "------------------------------------------------------------\n",
            "Q30: What London vehicle do Holmes and Watson frequently hire for short trips?\n",
            "A: A black cab.\n",
            "Question: What is\n",
            "Expected:A hansom cab.\n",
            "------------------------------------------------------------\n",
            "Q31: In which English county do Holmes and Watson investigate a deadly household powder?\n",
            "A: London\n",
            "Question: In which English county do\n",
            "Expected:Cornwall.\n",
            "------------------------------------------------------------\n",
            "Q32: Where does Holmes retire?\n",
            "A: Sherlock Holmes is a British actor, writer\n",
            "Expected:Sussex Downs.\n",
            "------------------------------------------------------------\n",
            "Q33: What does Watson call Holmes’ method of reasoning?\n",
            "A: \"The Sherlock Holmes Method of Reasoning\"\n",
            "Expected:“The science of deduction and analysis.”\n",
            "------------------------------------------------------------\n",
            "Q34: What is the name of the street‑boy network Holmes employs?\n",
            "A: \"The Sherlock Holmes Network\".\n",
            "Question:\n",
            "Expected:The Baker Street Irregulars.\n",
            "------------------------------------------------------------\n",
            "Q35: What London club is Mycroft Holmes most associated with?\n",
            "A: The Sherlock Holmes Club.\n",
            "Question: What\n",
            "Expected:The Diogenes Club.\n",
            "------------------------------------------------------------\n",
            "Q36: What is the name of the ancient document recited by Reginald Musgrave?\n",
            "A: \"The Book of the Law\"\n",
            "Question\n",
            "Expected:The Musgrave Ritual.\n",
            "------------------------------------------------------------\n",
            "Q37: What injury does Victor Hatherley suffer during his adventure?\n",
            "A: Victor Hatherley is injured in the\n",
            "Expected:Loss of a thumb.\n",
            "------------------------------------------------------------\n",
            "Q38: On what moor do the Baskerville events occur?\n",
            "A: The Baskerville events occur in the M\n",
            "Expected:Dartmoor.\n",
            "------------------------------------------------------------\n",
            "Q39: What substance is used to make an animal appear ghostly?\n",
            "A: A substance that is used to make an animal\n",
            "Expected:Phosphorus.\n",
            "------------------------------------------------------------\n",
            "Q40: In which county is Dartmoor located?\n",
            "A: Dartmoor\n",
            "Question: In which\n",
            "Expected:Devonshire.\n",
            "------------------------------------------------------------\n",
            "Q41: What animal do Holmes and Watson sometimes use to track a scent?\n",
            "A: A dog.\n",
            "Question: What is the\n",
            "Expected:A dog.\n",
            "------------------------------------------------------------\n",
            "Q42: What scientific field is Holmes notably skilled in?\n",
            "A: Science.\n",
            "Question: What is the most\n",
            "Expected:Chemistry.\n",
            "------------------------------------------------------------\n",
            "Q43: Which police force does Holmes frequently assist?\n",
            "A: The Sherlock Holmes Universe  is\n",
            "Expected:Scotland Yard.\n",
            "------------------------------------------------------------\n",
            "Q44: What railway timetable does Holmes often consult?\n",
            "A: \"The train timetable is a very important part\n",
            "Expected:Bradshaw's.\n",
            "------------------------------------------------------------\n",
            "Q45: What supernatural creature is suspected in a case involving a South American family?\n",
            "A: The answer is Sherlock Holmes.\n",
            "Question:\n",
            "Expected:A vampire.\n",
            "------------------------------------------------------------\n",
            "Q46: What quick message service does Holmes often use?\n",
            "A: \"I'm sorry, I'm busy.\"\n",
            "Expected:Telegrams.\n",
            "------------------------------------------------------------\n",
            "Q47: What handheld tool does Holmes use to inspect tiny clues?\n",
            "A: A small, round, metal tool called a\n",
            "Expected:A magnifying glass.\n",
            "------------------------------------------------------------\n",
            "Q48: In which city do most of Holmes's cases take place?\n",
            "A: London.\n",
            "Question: In which city do\n",
            "Expected:London.\n",
            "------------------------------------------------------------\n",
            "Q49: After retirement, what hobby does Holmes pursue?\n",
            "A: I'm a writer.\n",
            "Question: What\n",
            "Expected:Beekeeping.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "questions = \"data/questions_sherlock.json\"\n",
        "out_file_name = \"gpt2_answers.txt\"\n",
        "\n",
        "results = test_on_questions(prompt_builder=build_prompt, model_path=model_name, question_file=questions, out_file_name=out_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4e39ad2227eab",
      "metadata": {
        "id": "ef4e39ad2227eab"
      },
      "source": [
        "## 4. Analyse des résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7216ca",
      "metadata": {
        "id": "fe7216ca"
      },
      "source": [
        "### 4.1 Évaluation quantitative (à compléter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "47164745",
      "metadata": {
        "id": "47164745"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "102eef37",
      "metadata": {
        "id": "102eef37"
      },
      "outputs": [],
      "source": [
        "def evaluate_f1(ground_truth, prediction):\n",
        "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en commun et estime précision, rappel et F1.\"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3a838018",
      "metadata": {
        "id": "3a838018"
      },
      "outputs": [],
      "source": [
        "def evaluation_generation(results):\n",
        "    eval = {\"precision\":0, \"recall\":0, \"f1\":0}\n",
        "    for i, question, answer, expected_answer in results:\n",
        "        precision, recall, f1 = evaluate_f1(expected_answer, answer)\n",
        "        eval[\"precision\"] += precision\n",
        "        eval[\"recall\"] += recall\n",
        "        eval[\"f1\"] += f1\n",
        "\n",
        "    eval[\"precision\"] /= len(results)\n",
        "    eval[\"recall\"] /= len(results)\n",
        "    eval[\"f1\"] /= len(results)\n",
        "\n",
        "    return eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3802f19e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3802f19e",
        "outputId": "ce7dfb6b-904c-4aa2-bced-dee1ad95a3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.055523809523809524, 'recall': 0.145, 'f1': 0.07263636363636362}\n"
          ]
        }
      ],
      "source": [
        "eval = evaluation_generation(results)\n",
        "print(eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07bd46fb",
      "metadata": {
        "id": "07bd46fb"
      },
      "source": [
        "**Question:** Que pensez-vous de cette évaluation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83de8cb0",
      "metadata": {
        "id": "83de8cb0"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5288707a",
      "metadata": {
        "id": "5288707a"
      },
      "source": [
        "### 4. Analyse qualitative (à faire)\n",
        "\n",
        "Rédigez **5 à 8 phrases** expliquant ce que vous observez et pourquoi, selon vous, le modèle fournit ce type de réponses.\n",
        "\n",
        "> Cette étape prépare le terrain pour les tâches 3 et 4.\n",
        "> Il est normal que les réponses ne soient pas très bonnes à ce stade. On vous demande d’expliquer **pourquoi**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f095be4",
      "metadata": {
        "id": "1f095be4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "958676b4",
      "metadata": {
        "id": "958676b4"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}