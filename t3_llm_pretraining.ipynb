{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d53d4c2",
      "metadata": {
        "id": "4d53d4c2"
      },
      "source": [
        "# Tâche 3 : Question-réponse avec GPT‑2 avec poursuite du pré-entraînement sur un corpus de Sherlock Holmes\n",
        "\n",
        "**Objectifs**\n",
        "\n",
        "Évaluer la qualité des réponses d’un modèle de langage **pré‑entraîné** (version **GPT-2 Medium (355M)** sur Hugging Face) lorsqu’on lui pose des questions sur un sujet vu au pré-entraînement.\n",
        "Dans ce *notebook*, vous poursuivez le pré-entraînement du modèle avec des textes de l'univers de *Sherlock Holmes*.  Comme pour la tâche 2, on doit construire un *prompt* minimal, générer des réponses avec le nouveau modèle, et évaluer la pertinence des résultats. Plusieurs de ces fonctions sont rendues disponibles.\n",
        "\n",
        "**Objectifs d’apprentissage**\n",
        "1. Poursuivre le préentraînement d'un modèle pré‑entraîné (Hugging Face) sur un corpus de taille moyenne.\n",
        "2. Comprendre et expliquer les **limites et apports du pré‑entraînement** sur des textes pertinents au domaine des questions.\n",
        "\n",
        "Tout comme pour la tâche 2, les **questions** pour évaluer le modèle vous sont fournies. Vous devez comprendre le format des questions chargées en mémoire. La **liste de livres** à utiliser pour le pré-entraînement et la **fonction** pour les monter en mémoire sont également disponibles.\n",
        "\n",
        "NOTE: Il est important de sauvegarder le modèle pré-entraîné dans cette tâche (ainsi que son tokenizer) car nous les réutilisons pour la tâche 4.\n",
        "\n",
        "> Il est recommandé de faire ce travail pratique en utilisant une carte graphique GPU compatible avec HuggingFace/Pytorch.\n",
        "> Si votre machine n’en possède pas, vous pouvez utiliser **Google Colab** pour exécuter le *notebook* dans le cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81af0ea3655c4578",
      "metadata": {
        "id": "81af0ea3655c4578"
      },
      "source": [
        "Si nécessaire, installer les *packages* suivant. Si vous exécutez sur Code Colab, ces *packages* devraient déjà être installés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "P1jvDsOV7WVW",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:16:43.735114Z",
          "start_time": "2025-10-09T01:16:43.721585Z"
        },
        "id": "P1jvDsOV7WVW"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install accelerate\n",
        "#!pip install 'transformers[torch]'\n",
        "#!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4d460d1d84efc873",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:16:43.969814Z",
          "start_time": "2025-10-09T01:16:43.965774Z"
        },
        "id": "4d460d1d84efc873"
      },
      "outputs": [],
      "source": [
        "batch_size = 5 # il est possible d'ajuster la taille de batch. Les valeurs actuelles utilisent environ 10 Gb\n",
        "max_length = 256 # on réduit le contexte pour sauver du temps, nos exemples ne nécessite pas un plus grand contexte\n",
        "model_name = \"gpt2-medium\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "660a299e09c87174",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:16:55.143183Z",
          "start_time": "2025-10-09T01:16:44.018837Z"
        },
        "id": "660a299e09c87174"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import pipeline, Trainer\n",
        "import os\n",
        "import json\n",
        "\n",
        "import re\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "#Mount a google drive folder to save things\n",
        "if IN_COLAB:\n",
        "  drive.mount('/content/drive')\n",
        "  folders_to_mount = [\"nlp_tp2_models\", \"results\"]\n",
        "  for folder in folders_to_mount:\n",
        "    source = f'/content/drive/MyDrive/uni/nlp/{folder}'\n",
        "    shortcut = f'/content/{folder}'\n",
        "    print(f\"Mounting {source} to {shortcut}\")\n",
        "    os.symlink(source, shortcut)\n",
        "\n",
        "repo_url = \"https://github.com/XavyShmore/tp2_nlp.git\"\n",
        "if IN_COLAB:\n",
        "  !git clone {repo_url}\n",
        "  !cp -r ./tp2_nlp/data .\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "bEXSz8yy3jpH",
        "outputId": "141d89aa-680b-4ae0-bf79-3bd9b07b7d0e"
      },
      "id": "bEXSz8yy3jpH",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounting /content/drive/MyDrive/uni/nlp/nlp_tp2_models to /content/nlp_tp2_models\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/drive/MyDrive/uni/nlp/nlp_tp2_models' -> '/content/nlp_tp2_models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3564091894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/{folder}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mounting {source} to {shortcut}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortcut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrepo_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/XavyShmore/tp2_nlp.git\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/MyDrive/uni/nlp/nlp_tp2_models' -> '/content/nlp_tp2_models'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d5ba36",
      "metadata": {
        "id": "a0d5ba36"
      },
      "source": [
        "## 1. Chargement du modèle Hugging Face et du tokenizer (à compléter)\n",
        "\n",
        "Complétez le corps de la fonction `load_model(model_path)` afin qu’elle :\n",
        "\n",
        "- charge le **tokenizer** et le **modèle** Hugging Face à partir du chemin `model_path`.\n",
        "- **retourne** le tokenizer comme **première valeur de retour** et le modèle comme **seconde valeur de retour**.\n",
        "\n",
        "On ajoute également des fonctions pour monter les questions en mémoire et pour sauvegarder les réponses dans un fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "54a6b2f6",
      "metadata": {
        "id": "54a6b2f6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def load_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "      tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4c56a1e65ebeb313",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:16:55.164694Z",
          "start_time": "2025-10-09T01:16:55.156561Z"
        },
        "id": "4c56a1e65ebeb313"
      },
      "outputs": [],
      "source": [
        "def load_entries(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(f\"Question file must contain a list of objects. Got: {type(data)}\")\n",
        "    return data\n",
        "\n",
        "def save_answers(questions_answers, output_dir, out_file_name, display=True):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(os.path.join(output_dir, out_file_name), \"w\", encoding=\"utf-8\") as out:\n",
        "        for index, question, answer, expected_answer in questions_answers:\n",
        "            out.write(f\"Q: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\\n\")\n",
        "            if display:\n",
        "                print(f\"Q{index}: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc746173",
      "metadata": {
        "id": "bc746173"
      },
      "source": [
        "## 2. Fonctions de test question-réponse  (à compléter)\n",
        "\n",
        "La fonction **test_on_questions** est utilisée pour parcourir **toutes les entrées** du fichier de questions afin de produire des réponses générées par le modèle.\n",
        "\n",
        "La génération d'une réponse à une question implique les étapes suivantes (fonction **process_entry** à compléter) :\n",
        "* Construire un prompt à l’aide de la fonction **build_prompt** (rendu disponible).\n",
        "* Utiliser le modèle (via un pipeline de génération de texte passé en argument) pour générer une réponse à une question.\n",
        "* Retourner la réponse générée par le modèle.  \n",
        "\n",
        "Points importants à souligner:\n",
        "* La fonction *process_entry* doit retourner uniquement la réponse générée par le modèle (sans le prompt).\n",
        "* Il est de votre responsabililté de choisir **les paramètres** du générateur (max_new_tokens, do_sample, temperature, top_k ou top_p). Décrivez ceux que vous avez retenus.\n",
        "\n",
        "> Afin de simplifier le travail, nous avons choisi de ne pas utiliser de *batchs* dans la fonction qui teste les questions.\n",
        "> Vous n'avez pas à prendre en compte le *warning* qui suggère d'utiliser des *datasets*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "69ee0ec8",
      "metadata": {
        "id": "69ee0ec8"
      },
      "outputs": [],
      "source": [
        "def build_prompt(entry):\n",
        "    question = entry.get(\"question\", \"\")\n",
        "    prompt = f\"Please answer the following question about the Sherlock Holmes Universe with a short answer.\\nQuestion: {question}\\nAnswer: \"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc8d736",
      "metadata": {
        "id": "9fc8d736"
      },
      "source": [
        "Description des paramètres de génération:\n",
        "(à compléter...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "83653b6bd010065",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:16:55.183765Z",
          "start_time": "2025-10-09T01:16:55.175733Z"
        },
        "id": "83653b6bd010065"
      },
      "outputs": [],
      "source": [
        "def process_entry(entry, prompt_builder, generator):\n",
        "    prompt = prompt_builder(entry)\n",
        "    generation_output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=10, # limit the length of the generated answer\n",
        "        do_sample=False,    # enable sampling for more varied responses\n",
        "        temperature=0.8,   # control the randomness of the output\n",
        "        top_k=50,          # consider only the top k most likely next tokens\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    # The generated_text includes the prompt, so we need to extract only the answer\n",
        "    generated_text = generation_output[0]['generated_text']\n",
        "    answer = generated_text[len(prompt):].strip()\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b457b86d",
      "metadata": {
        "id": "b457b86d"
      },
      "outputs": [],
      "source": [
        "def test_on_questions(prompt_builder, model_path, question_file, out_file_name, output_dir=\"results\"):\n",
        "    entries = load_entries(question_file)\n",
        "    tokenizer, model = load_model(model_path)\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    results = []\n",
        "    for i, entry in enumerate(entries):\n",
        "        answer = process_entry(entry, prompt_builder, generator)\n",
        "        question = entry.get(\"question\", \"\")\n",
        "        expected_answer = entry.get(\"answer\", \"\")\n",
        "        results.append((i, question, answer, expected_answer))\n",
        "    save_answers(results, output_dir, out_file_name, display=True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c70fe96",
      "metadata": {
        "id": "6c70fe96"
      },
      "source": [
        "## 3. Poursuite du pré-entraînement du modèle GPT-2 (à compléter)\n",
        "\n",
        "Complétez le code suivant afin de poursuivre le préentraînement de GPT2 sur des textes de *Sherlock Holmes*.\n",
        "\n",
        "Les étapes à suivre sont de :\n",
        "* Télécharger le contenu des livres (on rend la fonction disponible)\n",
        "* Créer un *dataset* (version Hugging Face) d'entraînement à partir de ce contenu\n",
        "* Tokeniser ce *dataset*\n",
        "* Faire le pré-entraînement du modèle sur le *dataset* avec la classe ***Trainer*** de Hugging Face\n",
        "* Faire la sauvegarde du nouveau modèle dans un répertoire (voir *model_path*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "67b14a6bcc8ee527",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:16:59.580570Z",
          "start_time": "2025-10-09T01:16:55.197091Z"
        },
        "id": "67b14a6bcc8ee527"
      },
      "outputs": [],
      "source": [
        "books = {\n",
        "    \"The Sign of the Four\": \"https://www.gutenberg.org/files/2097/2097-0.txt\",\n",
        "    \"The Adventures of Sherlock Holmes\": \"https://www.gutenberg.org/files/1661/1661-0.txt\",\n",
        "    \"The Memoirs of Sherlock Holmes\": \"https://www.gutenberg.org/files/834/834-0.txt\",\n",
        "    \"The Hound of the Baskervilles\": \"https://www.gutenberg.org/files/2852/2852-0.txt\",\n",
        "    \"His Last Bow\": \"https://www.gutenberg.org/files/2350/2350-0.txt\",\n",
        "    \"The Case-Book of Sherlock Holmes\": \"https://www.gutenberg.org/files/221/221-0.txt\"\n",
        "}\n",
        "\n",
        "def download_sherlock_dataset(books_to_process):\n",
        "    data = []\n",
        "\n",
        "    for title, url in books_to_process.items():\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            text = response.text\n",
        "\n",
        "            # Cette expression est plus robuste que ce qui était attendu dans le premier travail pratique\n",
        "            header_regex = r\"(?s)^.*?\\*{3}\\s*START OF\\b.*?\\r?\\n?\\*{3}\\s*\\r?\\n\"\n",
        "            header_pattern = re.compile(header_regex, flags=0)\n",
        "            clean_text = header_pattern.sub(\"\", text)\n",
        "\n",
        "            toc_regex = r\"(?ims)^\\s*contents\\s*$.*?^\\s*$\"\n",
        "            toc_pattern = re.compile(toc_regex, flags=0)\n",
        "            clean_text = toc_pattern.sub(\"\", clean_text)\n",
        "\n",
        "            regex_license_llm = r\"(?im)^\\s*\\*{3} END OF\\b.*[\\s\\S]*\\Z\"\n",
        "            license_llm_pattern = re.compile(regex_license_llm, flags=0)\n",
        "            clean_text = license_llm_pattern.sub(\"\", clean_text)\n",
        "\n",
        "            # To make it simpler to learn text without learning new lines\n",
        "            clean_text = re.sub(r\"\\r\\n\\r\\n\", \"\\n\", clean_text)\n",
        "            clean_text = re.sub(r\"\\r\\n\", \" \", clean_text)\n",
        "\n",
        "            #retirer white doubled white char\n",
        "            clean_text = re.sub(r'\\s{2,}', ' ', clean_text)\n",
        "\n",
        "            data.append(clean_text)\n",
        "            print(f\"Downloaded: {title}\")\n",
        "        else:\n",
        "            print(f\"Failed to download: {title}\")\n",
        "\n",
        "    return \"\\n\".join(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3938df",
      "metadata": {
        "id": "9e3938df"
      },
      "source": [
        "Création du *dataset* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "87ffcebd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "87ffcebd",
        "outputId": "fe8989d2-1592-42df-9800-a0d2fe2ee878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: The Sign of the Four\n",
            "Downloaded: The Adventures of Sherlock Holmes\n",
            "Downloaded: The Memoirs of Sherlock Holmes\n",
            "Downloaded: The Hound of the Baskervilles\n",
            "Downloaded: His Last Bow\n",
            "Downloaded: The Case-Book of Sherlock Holmes\n"
          ]
        }
      ],
      "source": [
        "sherlock_text = download_sherlock_dataset(books_to_process=books)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(text, tokeniser, context_length):\n",
        "  tokenizer_max_length = tokeniser.model_max_length\n",
        "\n",
        "  tokeniser.model_max_length = 1000000 # Mentire au tokeniser pour qu'il fonctionne même si on dépasse la limite du modèle\n",
        "  tokenized_text = tokeniser(sherlock_text)\n",
        "  tokeniser.model_max_length = tokenizer_max_length # Rétablir la vraie valeur\n",
        "\n",
        "  text_token_length = len(tokenized_text[\"input_ids\"])\n",
        "  print(f\"Generating {text_token_length} tokens\")\n",
        "\n",
        "  input_ids = [tokenized_text[\"input_ids\"][i*context_length:(i+1)*context_length] for i in range(text_token_length // context_length)]\n",
        "  attention_mask = [tokenized_text[\"attention_mask\"][i*context_length:(i+1)*context_length] for i in range(text_token_length // context_length)]\n",
        "\n",
        "  return Dataset.from_dict({\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_mask\n",
        "  })\n",
        "\n"
      ],
      "metadata": {
        "id": "sZVTssYb8DKK"
      },
      "id": "sZVTssYb8DKK",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b9a1e7a7",
      "metadata": {
        "id": "b9a1e7a7"
      },
      "source": [
        "Tokénisation du *dataset* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1790b1cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1790b1cc",
        "outputId": "0dcfb3ac-8a2a-4d18-c1d7-4fa5e91623ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 639546 tokens\n"
          ]
        }
      ],
      "source": [
        "tokenizer, model = load_model(model_name)\n",
        "\n",
        "tokenized_dataset = create_dataset(sherlock_text, tokenizer, max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5779a3e2",
      "metadata": {
        "id": "5779a3e2"
      },
      "source": [
        "Ajouter dans les cellules suivantes le code dont vous avez besoin pour poursuivre le pré-entraînement du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8792c5a4",
      "metadata": {
        "id": "8792c5a4"
      },
      "outputs": [],
      "source": [
        "model_path = \"/nlp_tp2_models/gpt2/gpt2-sherlock-lm\"  # Répertoire où sauvegarder le nouveau modèle et le tokenizer\n",
        "\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nlp_tp2_models/gpt2/gpt2-sherlock-lm/checkpoints\",\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    num_train_epochs=3,\n",
        "\n",
        "    # 2. BATCH SIZE: Small dataset = requires stability.\n",
        "    # We use gradient accumulation to simulate a larger batch size (e.g., 32)\n",
        "    # while keeping memory usage low.\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4, # Effective batch size = 4 * 8 = 32\n",
        "\n",
        "    # 3. LEARNING RATE: The most critical part.\n",
        "    # Standard is 5e-5. Since your data is small, sticking to the lower end\n",
        "    # prevents destroying the pre-trained knowledge.\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # 4. SCHEDULER: vital for small data\n",
        "    warmup_steps=50, # Warm up quickly (roughly 1 epoch worth of steps)\n",
        "    lr_scheduler_type=\"cosine\", # Smooth decay is better than linear for language\n",
        "\n",
        "    # 5. LOGGING\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    fp16=True, # Use mixed precision if on GPU (much faster)\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6fa0cf15",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:46:46.257446Z",
          "start_time": "2025-10-09T01:16:59.657914Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "6fa0cf15",
        "outputId": "6a5eb233-ece5-413f-df97-ebc9394d4924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [237/237 19:16, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.180100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.092400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.956800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.955600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.937600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.883100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.807700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.792600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.796200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.786800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.793700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.784800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.804200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.702800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.709500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.714100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.678100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.693300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.685400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.700200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=237, training_loss=2.845058924035181, metrics={'train_runtime': 1158.7502, 'train_samples_per_second': 6.467, 'train_steps_per_second': 0.205, 'total_flos': 3479841502396416.0, 'train_loss': 2.845058924035181, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9efb4a",
      "metadata": {
        "id": "dd9efb4a"
      },
      "source": [
        "Pour conclure cette section, sauvegardez le nouveau modèle et le *tokenizer* afin de les réutiliser dans la tâche 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "7d73409d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d73409d",
        "outputId": "659003c3-0637-400d-ede1-d32bebf4f7ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/nlp_tp2_models/gpt2/gpt2-sherlock-lm/tokenizer_config.json',\n",
              " '/nlp_tp2_models/gpt2/gpt2-sherlock-lm/special_tokens_map.json',\n",
              " '/nlp_tp2_models/gpt2/gpt2-sherlock-lm/vocab.json',\n",
              " '/nlp_tp2_models/gpt2/gpt2-sherlock-lm/merges.txt',\n",
              " '/nlp_tp2_models/gpt2/gpt2-sherlock-lm/added_tokens.json',\n",
              " '/nlp_tp2_models/gpt2/gpt2-sherlock-lm/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabd806c",
      "metadata": {
        "id": "dabd806c"
      },
      "source": [
        "## 4. Génération de réponses avec le nouveau modèle GPT-2\n",
        "\n",
        "Exécutez la cellule suivante pour générer les réponses aux questions avec le modèle que vous venez de pré-entraîner sur des textes de *Sherlock Holmes*.\n",
        "Le temps d’exécution devrait se situer entre **5 et 10 minutes** si vous utilisez **Google Colab** avec un GPU.\n",
        "\n",
        "Note : N'oubliez pas d'ajouter le fichier de réponses générées par le modèle (voir *out_file_name*) dans votre remise du travail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "wA5PwWtdQvNE",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-09T01:54:40.206982Z",
          "start_time": "2025-10-09T01:46:46.318579Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA5PwWtdQvNE",
        "outputId": "69d863ca-625b-40c6-ab30-357bc0f476e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q0: Where do Sherlock Holmes and Dr. Watson live?\n",
            "A: In the house of Sir Henry Carey, at\n",
            "Expected:221B Baker Street, London.\n",
            "------------------------------------------------------------\n",
            "Q1: Who is Sherlock Holmes' loyal friend and chronicler?\n",
            "A: I am not sure.”\n",
            "�\n",
            "Expected:Dr. John H. Watson.\n",
            "------------------------------------------------------------\n",
            "Q2: Who is considered 'The Woman' by Sherlock Holmes?\n",
            "A: The woman is a woman who has been married\n",
            "Expected:Irene Adler.\n",
            "------------------------------------------------------------\n",
            "Q3: Which story features the Red-Headed League?\n",
            "A: The Red-Headed League.\n",
            "�\n",
            "Expected:The Adventure of the Red-Headed League.\n",
            "------------------------------------------------------------\n",
            "Q4: What is the primary occupation of Sherlock Holmes?\n",
            "A: A detective.\n",
            "“What is the\n",
            "Expected:Consulting detective.\n",
            "------------------------------------------------------------\n",
            "Q5: Who is Sherlock Holmes' arch-nemesis?\n",
            "A: The man who has been the most formidable enemy\n",
            "Expected:Professor James Moriarty.\n",
            "------------------------------------------------------------\n",
            "Q6: What musical instrument does Sherlock Holmes play?\n",
            "A: A violin.\n",
            "“What is the\n",
            "Expected:The violin.\n",
            "------------------------------------------------------------\n",
            "Q7: What is the name of Sherlock Holmes' elder brother?\n",
            "A: Sherlock Holmes.\n",
            "“‘\n",
            "Expected:Mycroft Holmes.\n",
            "------------------------------------------------------------\n",
            "Q8: What is the blue gemstone found inside a Christmas goose called?\n",
            "A: A small blue gemstone, about the size\n",
            "Expected:The Blue Carbuncle.\n",
            "------------------------------------------------------------\n",
            "Q9: What residue does Holmes often analyze to identify smokers?\n",
            "A: A very small amount.”\n",
            "�\n",
            "Expected:Tobacco ash.\n",
            "------------------------------------------------------------\n",
            "Q10: What tactic besides observation does Holmes often use to gather information?\n",
            "A: I have no idea.”\n",
            "�\n",
            "Expected:Disguise.\n",
            "------------------------------------------------------------\n",
            "Q11: What was the profession of Dr. John Watson?\n",
            "A: A doctor.\n",
            "“‘A\n",
            "Expected:Doctor.\n",
            "------------------------------------------------------------\n",
            "Q12: What kind of marks on the ground does Holmes often study to track people?\n",
            "A: A. The marks of a man who has\n",
            "Expected:Footprints.\n",
            "------------------------------------------------------------\n",
            "Q13: Who is the landlady of 221B Baker Street?\n",
            "A: Mrs. Mary Baker St. Clair.\n",
            "Expected:Mrs. Hudson.\n",
            "------------------------------------------------------------\n",
            "Q14: What substance did Sherlock Holmes sometimes use to stimulate his mind?\n",
            "A: A little bit of opium, a little bit\n",
            "Expected:Cocaine.\n",
            "------------------------------------------------------------\n",
            "Q15: Who is the Scotland Yard detective that often consults Holmes?\n",
            "A: The Inspector of Constabulary.\n",
            "“\n",
            "Expected:Inspector Lestrade (Gregson or Bradstreet also acceptable).\n",
            "------------------------------------------------------------\n",
            "Q16: What warning arrives as envelopes containing dried orange seeds?\n",
            "A: A small one.\n",
            "“What is\n",
            "Expected:Five orange pips.\n",
            "------------------------------------------------------------\n",
            "Q17: What is the bog near Baskerville Hall called?\n",
            "A: Baskerville Hall.\n",
            "“�\n",
            "Expected:The Grimpen Mire.\n",
            "------------------------------------------------------------\n",
            "Q18: Which London newspaper does Holmes frequently read?\n",
            "A: The Daily Telegraph.\n",
            "“‘\n",
            "Expected:The Times.\n",
            "------------------------------------------------------------\n",
            "Q19: What kind of drawings form the code that Holmes deciphers on walls and paper?\n",
            "A: A drawing of a man with a stick in\n",
            "Expected:Dancing stick figures.\n",
            "------------------------------------------------------------\n",
            "Q20: What kind of jewel is set in the damaged coronet handled by a banker?\n",
            "A: A small, round, black stone.\n",
            "Expected:Beryls.\n",
            "------------------------------------------------------------\n",
            "Q21: What combat sport is Holmes proficient in?\n",
            "A: A very simple one. I have been a\n",
            "Expected:Boxing.\n",
            "------------------------------------------------------------\n",
            "Q22: What does Sherlock Holmes keep in his Persian slipper?\n",
            "A: A small box of cigarettes.\n",
            "“\n",
            "Expected:Pipe tobacco.\n",
            "------------------------------------------------------------\n",
            "Q23: In which room at 221B do most client interviews happen?\n",
            "A: In the office of the Inspector of Constabulary\n",
            "Expected:The sitting-room.\n",
            "------------------------------------------------------------\n",
            "Q24: What important document goes missing from the Foreign Office in one case?\n",
            "A: The _Daily Telegraph_.\n",
            "“\n",
            "Expected:A secret naval treaty.\n",
            "------------------------------------------------------------\n",
            "Q25: What hot drink is often served at Baker Street?\n",
            "A: iced coffee.\n",
            "“‘What is\n",
            "Expected:Tea.\n",
            "------------------------------------------------------------\n",
            "Q26: What object ties Irene Adler to a scandal with a European king?\n",
            "A: The death of the King of Bohemia.\n",
            "Expected:A compromising photograph.\n",
            "------------------------------------------------------------\n",
            "Q27: What weapon does Dr. Watson often carry?\n",
            "A: A revolver.\n",
            "“What is the\n",
            "Expected:A revolver.\n",
            "------------------------------------------------------------\n",
            "Q28: In which country did Dr. Watson serve as an army doctor?\n",
            "A: In the United States.\n",
            "“�\n",
            "Expected:Afghanistan.\n",
            "------------------------------------------------------------\n",
            "Q29: What is the missing racehorse’s name in the Dartmoor case?\n",
            "A: “The Hound of the Baskervilles\n",
            "Expected:Silver Blaze.\n",
            "------------------------------------------------------------\n",
            "Q30: What London vehicle do Holmes and Watson frequently hire for short trips?\n",
            "A: A black cab.\n",
            "“‘\n",
            "Expected:A hansom cab.\n",
            "------------------------------------------------------------\n",
            "Q31: In which English county do Holmes and Watson investigate a deadly household powder?\n",
            "A: In the county of Berkshire.\n",
            "“\n",
            "Expected:Cornwall.\n",
            "------------------------------------------------------------\n",
            "Q32: Where does Holmes retire?\n",
            "A: From the police.\n",
            "“What is\n",
            "Expected:Sussex Downs.\n",
            "------------------------------------------------------------\n",
            "Q33: What does Watson call Holmes’ method of reasoning?\n",
            "A: “The method of reasoning which he employs is\n",
            "Expected:“The science of deduction and analysis.”\n",
            "------------------------------------------------------------\n",
            "Q34: What is the name of the street‑boy network Holmes employs?\n",
            "A: The name is Sherlock Holmes.\n",
            "“\n",
            "Expected:The Baker Street Irregulars.\n",
            "------------------------------------------------------------\n",
            "Q35: What London club is Mycroft Holmes most associated with?\n",
            "A: The Metropolitan Police.\n",
            "“What is\n",
            "Expected:The Diogenes Club.\n",
            "------------------------------------------------------------\n",
            "Q36: What is the name of the ancient document recited by Reginald Musgrave?\n",
            "A: The Ritual of the Ritual of the Ritual of\n",
            "Expected:The Musgrave Ritual.\n",
            "------------------------------------------------------------\n",
            "Q37: What injury does Victor Hatherley suffer during his adventure?\n",
            "A: I have no idea.”\n",
            "�\n",
            "Expected:Loss of a thumb.\n",
            "------------------------------------------------------------\n",
            "Q38: On what moor do the Baskerville events occur?\n",
            "A: On the moor of the Isle of W\n",
            "Expected:Dartmoor.\n",
            "------------------------------------------------------------\n",
            "Q39: What substance is used to make an animal appear ghostly?\n",
            "A: A substance which is used to make a man\n",
            "Expected:Phosphorus.\n",
            "------------------------------------------------------------\n",
            "Q40: In which county is Dartmoor located?\n",
            "A: Dartmoor.\n",
            "“�\n",
            "Expected:Devonshire.\n",
            "------------------------------------------------------------\n",
            "Q41: What animal do Holmes and Watson sometimes use to track a scent?\n",
            "A: A dog.\n",
            "“‘A\n",
            "Expected:A dog.\n",
            "------------------------------------------------------------\n",
            "Q42: What scientific field is Holmes notably skilled in?\n",
            "A: The study of the chemical composition of matter.\n",
            "Expected:Chemistry.\n",
            "------------------------------------------------------------\n",
            "Q43: Which police force does Holmes frequently assist?\n",
            "A: The Metropolitan Police.\n",
            "“‘\n",
            "Expected:Scotland Yard.\n",
            "------------------------------------------------------------\n",
            "Q44: What railway timetable does Holmes often consult?\n",
            "A: The London Underground.\n",
            "“‘\n",
            "Expected:Bradshaw's.\n",
            "------------------------------------------------------------\n",
            "Q45: What supernatural creature is suspected in a case involving a South American family?\n",
            "A: A man who has been in the habit of\n",
            "Expected:A vampire.\n",
            "------------------------------------------------------------\n",
            "Q46: What quick message service does Holmes often use?\n",
            "A: The London Underground.\n",
            "“‘\n",
            "Expected:Telegrams.\n",
            "------------------------------------------------------------\n",
            "Q47: What handheld tool does Holmes use to inspect tiny clues?\n",
            "A: A small, thin, black-and-\n",
            "Expected:A magnifying glass.\n",
            "------------------------------------------------------------\n",
            "Q48: In which city do most of Holmes's cases take place?\n",
            "A: London.\n",
            "“‘‘\n",
            "Expected:London.\n",
            "------------------------------------------------------------\n",
            "Q49: After retirement, what hobby does Holmes pursue?\n",
            "A: I have no particular interest in the history of\n",
            "Expected:Beekeeping.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "questions = \"data/questions_sherlock.json\"\n",
        "out_file_name = \"pretrained_gpt2_answers.txt\"\n",
        "\n",
        "results = test_on_questions(prompt_builder=build_prompt, model_path=model_path, question_file=questions, out_file_name=out_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d73c5cc",
      "metadata": {
        "id": "2d73c5cc"
      },
      "source": [
        "## 5. Analyse des résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8e187a",
      "metadata": {
        "id": "5d8e187a"
      },
      "source": [
        "### 5.1 Évaluation quantitative (à compléter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7a7a7226",
      "metadata": {
        "id": "7a7a7226"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "89d80fa7",
      "metadata": {
        "id": "89d80fa7"
      },
      "outputs": [],
      "source": [
        "def evaluate_f1(ground_truth, prediction):\n",
        "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en commun et estime précision, rappel et F1.\"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "eec8ec86",
      "metadata": {
        "id": "eec8ec86"
      },
      "outputs": [],
      "source": [
        "def evaluation_generation(results):\n",
        "    eval = {\"precision\":0, \"recall\":0, \"f1\":0}\n",
        "    for i, question, answer, expected_answer in results:\n",
        "        precision, recall, f1 = evaluate_f1(expected_answer, answer)\n",
        "        eval[\"precision\"] += precision\n",
        "        eval[\"recall\"] += recall\n",
        "        eval[\"f1\"] += f1\n",
        "\n",
        "    eval[\"precision\"] /= len(results)\n",
        "    eval[\"recall\"] /= len(results)\n",
        "    eval[\"f1\"] /= len(results)\n",
        "\n",
        "    return eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0044a2a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0044a2a4",
        "outputId": "e69a567c-47c0-4750-bbca-798f86406fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.10133333333333333, 'recall': 0.1861904761904762, 'f1': 0.12565223665223665}\n"
          ]
        }
      ],
      "source": [
        "eval = evaluation_generation(results)\n",
        "print(eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53cfb4d1",
      "metadata": {
        "id": "53cfb4d1"
      },
      "source": [
        "**Question :** Que pensez-vous de cette évaluation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ecc9e77",
      "metadata": {
        "id": "5ecc9e77"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb1b717",
      "metadata": {
        "id": "fbb1b717"
      },
      "source": [
        "## 5. Analyse qualitative (à faire)\n",
        "\n",
        "Rédigez **5 à 15 phrases** présentant vos observations et expliquant pourquoi, selon vous, le modèle fournit ce type de réponses.\n",
        "\n",
        "Vous pouvez ajouter des cellules au besoin.\n",
        "\n",
        "> Cette étape prépare le terrain pour la tâche 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533f01e3",
      "metadata": {
        "id": "533f01e3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c1affa",
      "metadata": {
        "id": "38c1affa"
      },
      "source": [
        "   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}