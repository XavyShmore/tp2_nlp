{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #1 : Prédiction du score NOVA d'aliments avec des modèles *Transformers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend, comme au premier travail, la tâche de prédiction du score NOVA de produits alimentaires. Cependant, des nouveaux jeux de données ont été produits. Le corpus de textes contient 3 partitions : \n",
    "-\tUn fichier d’entraînement -  *data/t1_nova_train.json*\n",
    "-\tUn fichier de validation -  *data/t1_nova_dev.json*\n",
    "-\tUn fichier de test - *data/t1_nova_test.json*\n",
    " \n",
    "Utilisez la librairie *HuggingFace* pour accomplir cette tâche. On vous demande plus spécifiquement d’utiliser 2 modèles: le modèle ***bert-base-uncased*** et un **modèle encodeur multilingue** de votre choix. \n",
    "\n",
    "Les consignes pour cette tâche sont : \n",
    "- Nom du notebook : *t1_classification_nova.ipynb* (ce notebook). \n",
    "- Tokenisation : Celle fournie par les tokeniseurs accompagnant les modèles transformers. \n",
    "- Plongements de mots : Ceux du modèle *transformer*. \n",
    "- Normalisation : Lettre en minuscule pour Bert (rien à faire, le tokenizer s’en occupe). Aucune contrainte pour le 2e modèle mais il est préférable de comparer les modèles sur une même base (lettres minuscules). \n",
    "- Choix du 2e transformer: Un modèle encodeur multilingue préentraîné. Le modèle peut être une variante de Bert, mais cela n'est pas exigé. Me consulter en cas de doute pour valider votre choix.\n",
    "- Entraînement : Un affinage (*fine-tuning*) des modèles encodeurs. Pas de pré-entraînement (*no further pretraining*) demandé pour cette tâche. \n",
    "- Analyse : Présentez clairement vos résultats et faites-en l’analyse. Comparez les résultats obtenus avec les 2 modèles.    \n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2, etc.) si cela améliore la lisibilité. \n",
    "\n",
    "Vous pouvez diviser cette tâche en 2 *notebooks* (un pour chaque modèle) si cela est plus simple pour vous. Dans ce cas, ajoutez le nom du modèle dans le nom du fichier (par ex. *t1_classification_nova_bert.ipynb*) et retirez les cellules prévues pour le 2e modèle.  \n",
    "\n",
    "Notes :\n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si non trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement des modèles.\n",
    "- Vous pouvez ajouter des cellules au *notebook* comme bon vous semble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nécessaire, installez les *packages* suivant. Si vous exécutez sur Code Colab, ces *packages* devraient déjà être installés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install accelerate\n",
    "#!pip install \"transformers[torch]\"\n",
    "#!pip3 install torch torchvision\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*les 3 partitions du dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_fn = \"./data/t1_nova_train.json\"  # Jeu de données d'entraînement\n",
    "dev_json_fn = \"./data/t1_nova_dev.json\"  # Jeu de données de validation\n",
    "test_json_fn = \"./data/t1_nova_test.json\"  # Jeu de données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "label2id = {\"Groupe 1\": 0, \"Groupe 2\": 1, \"Groupe 3\": 2, \"Groupe 4\": 3}\n",
    "id2label = {0: \"Groupe 1\", 1: \"Groupe 2\", 2: \"Groupe 3\", 3: \"Groupe 4\"}\n",
    "\n",
    "def load_data(json_fn):\n",
    "    dataset = Dataset.from_json(json_fn)\n",
    "    dataset = dataset.map(lambda nova_group: {\"labels\": label2id[nova_group[\"nova\"]]})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_data(train_json_fn)\n",
    "dev_dataset = load_data(dev_json_fn)\n",
    "test_dataset = load_data(test_json_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification des scores NOVA avec BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modèle BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_dataset(data_set, tokenizer):\n",
    "    return data_set.map(lambda examples: tokenizer(examples['text'], padding=\"max_length\", truncation=True))\n",
    "\n",
    "def tokenize_all_dataset(train_dataset, dev_dataset, test_dataset):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    train_dataset = tokenize_dataset(train_dataset, tokenizer)\n",
    "    dev_dataset = tokenize_dataset(dev_dataset, tokenizer)\n",
    "    test_dataset = tokenize_dataset(test_dataset, tokenizer)\n",
    "    return train_dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7651/7651 [00:02<00:00, 2718.88 examples/s]\n",
      "Map: 100%|██████████| 851/851 [00:00<00:00, 2768.11 examples/s]\n",
      "Map: 100%|██████████| 2126/2126 [00:00<00:00, 2761.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, dev_dataset, test_dataset = tokenize_all_dataset(train_dataset, dev_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Entraînement du modèle BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=4, id2label=id2label, label2id=label2id)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mon_modele_nova\",   # Dossier de sauvegarde\n",
    "    eval_strategy=\"epoch\",          # Évaluer à la fin de chaque époque\n",
    "    save_strategy=\"epoch\",          # Sauvegarder à la fin de chaque époque\n",
    "    learning_rate=2e-5,             # Vitesse d'apprentissage (standard pour BERT)\n",
    "    per_device_train_batch_size=8,  # Taille des lots (baisser si erreur mémoire GPU)\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,             # Nombre de fois qu'il voit toutes les données\n",
    "    weight_decay=0.01,              # Régularisation\n",
    "    load_best_model_at_end=True,    # Garder le meilleur modèle à la fin, pas le dernier\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dev_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Évaluation du modèle BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification des scores NOVA avec un encodeur multilingue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Deuxième modèle - Un encodeur multilingue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décrivez ici le modèle que vous avez choisi. (À compléter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entraînement du deuxième modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Évaluation du deuxième modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse des résultats et comparaison des 2 modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
