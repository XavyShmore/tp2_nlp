{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d53d4c2",
      "metadata": {
        "id": "4d53d4c2"
      },
      "source": [
        "# Tâche 4 : Question-réponse avec affinage par instructions du modèle GPT‑2 pré-entraîné sur Sherlock Holmes\n",
        "\n",
        "**Objectifs**\n",
        "\n",
        "Évaluer la qualité des réponses d’un modèle de langage **pré‑entraîné** (celui de la tâche 3) et affiner par instructions (cette tâche).\n",
        "Dans ce *notebook*, vous faites le post-entraînement du modèle avec des instructions générales indiquant au modèle comment accomplir des tâches simples. Comme pour les tâches 2 et 3, la démarche de test est de construire un *prompt*, de générer des réponses, et d'évaluer qualitativement la pertinence des résultats. Plusieurs de ces fonctions sont rendues disponibles.\n",
        "\n",
        "**Objectifs d’apprentissage**\n",
        "1. Faire le post-entraînement d'un modèle pré‑entraîné avec l'affinage par instructions (*instruction tuning*).\n",
        "2. Comprendre et expliquer les **limites et apports de l'affinage par instructions** d'un modèle.\n",
        "\n",
        "Tout comme dans les tâches 2 et 3, les **questions** pour évaluer le modèle vous sont fournies. Le fichier d'**instructions** pour l'affinage du modèle est également fourni. Vous devez comprendre le format des questions chargées en mémoire. Il est également important de prendre connaissance de la nature des instructions utilisées pour l'affinage.\n",
        "\n",
        "> Il est recommandé de faire ce travail pratique en utilisant une carte graphique GPU compatible avec HuggingFace/Pytorch.\n",
        "> Si votre machine n’en possède pas, vous pouvez utiliser **Google Colab** pour exécuter le *notebook* dans le cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81af0ea3655c4578",
      "metadata": {
        "id": "81af0ea3655c4578"
      },
      "source": [
        "Si nécessaire, installer les *packages* suivant. Si vous exécutez sur Code Colab, ces *packages* devraient déjà être installés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P1jvDsOV7WVW",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:22.883417Z",
          "start_time": "2025-10-19T23:15:22.872259Z"
        },
        "id": "P1jvDsOV7WVW"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install accelerate\n",
        "#!pip install 'transformers[torch]'\n",
        "#!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4d460d1d84efc873",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:23.130682Z",
          "start_time": "2025-10-19T23:15:23.125998Z"
        },
        "id": "4d460d1d84efc873"
      },
      "outputs": [],
      "source": [
        "batch_size = 5 # il est possible d'ajuster la taille de batch. Les valeurs actuelles utilisent environ 10 Gb\n",
        "max_length = 256 # on réduit le contexte pour sauver du temps, nos exemples ne nécesside pas un plus grand contexte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "660a299e09c87174",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.145468Z",
          "start_time": "2025-10-19T23:15:23.155231Z"
        },
        "id": "660a299e09c87174"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import pipeline, Trainer\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "#Mount a google drive folder to save things\n",
        "if IN_COLAB:\n",
        "  drive.mount('/content/drive')\n",
        "  folders_to_mount = [\"nlp_tp2_models\", \"results\"]\n",
        "  for folder in folders_to_mount:\n",
        "    source = f'/content/drive/MyDrive/uni/nlp/{folder}'\n",
        "    shortcut = f'/content/{folder}'\n",
        "    print(f\"Mounting {source} to {shortcut}\")\n",
        "    os.symlink(source, shortcut)\n",
        "\n",
        "repo_url = \"https://github.com/XavyShmore/tp2_nlp.git\"\n",
        "if IN_COLAB:\n",
        "  !git clone {repo_url}\n",
        "  !cp -r ./tp2_nlp/data .\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec798R5cXRSW",
        "outputId": "29a8f143-5524-4eaf-b01b-d6dff421f856"
      },
      "id": "ec798R5cXRSW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounting /content/drive/MyDrive/uni/nlp/nlp_tp2_models to /content/nlp_tp2_models\n",
            "Mounting /content/drive/MyDrive/uni/nlp/results to /content/results\n",
            "Cloning into 'tp2_nlp'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 31 (delta 13), reused 11 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 2.47 MiB | 8.42 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b911ebeb",
      "metadata": {
        "id": "b911ebeb"
      },
      "source": [
        "## 1. Chargement du modèle Hugging Face et du tokenizer (à compléter)\n",
        "\n",
        "Complétez le corps de la fonction `load_model(model_path)` afin qu’elle :\n",
        "\n",
        "- charge le **tokenizer** et le **modèle** Hugging Face à partir du chemin `model_path`.\n",
        "- **retourne** le tokenizer comme **première valeur de retour** et le modèle comme **seconde valeur de retour**.\n",
        "\n",
        "On ajoute également des fonctions pour monter les questions en mémoire et pour sauvegarder les réponses dans un fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f9254d41",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.160378Z",
          "start_time": "2025-10-19T23:15:34.155815Z"
        },
        "id": "f9254d41"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def load_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c56a1e65ebeb313",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.177800Z",
          "start_time": "2025-10-19T23:15:34.171427Z"
        },
        "id": "4c56a1e65ebeb313"
      },
      "outputs": [],
      "source": [
        "def load_entries(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(f\"Question file must contain a list of objects. Got: {type(data)}\")\n",
        "    return data\n",
        "\n",
        "def save_answers(questions_answers, output_dir, out_file_name, display=True):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(os.path.join(output_dir, out_file_name), \"w\", encoding=\"utf-8\") as out:\n",
        "        for index, question, answer, expected_answer in questions_answers:\n",
        "            out.write(f\"Q: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\\n\")\n",
        "            if display:\n",
        "                print(f\"Q{index}: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c168134c",
      "metadata": {
        "id": "c168134c"
      },
      "source": [
        "## 2. Fonctions de test question-réponse  (à compléter)\n",
        "\n",
        "La fonction **test_on_questions** est utilisée pour parcourir **toutes les entrées** du fichier de questions afin de produire des réponses générées par le modèle.\n",
        "\n",
        "La génération d'une réponse à une question implique les étapes suivantes (fonction **process_entry** à compléter) :\n",
        "* Construire un prompt à l’aide de la fonction **alpaca_build_prompt** (rendu disponible dans la prochaine section)\n",
        "* Utiliser le modèle (via un pipeline de génération de texte) pour générer une réponse à une question\n",
        "* Retourner la réponse générée par le modèle.  \n",
        "\n",
        "Points importants à souligner:\n",
        "* La fonction *process_entry* doit retourner uniquement la réponse générée par le modèle (sans le prompt).\n",
        "* Il est de votre responsabililté de choisir **les paramètres** du générateur (max_new_tokens, do_sample, temperature, top_k ou top_p). Décrivez ceux que vous avez retenus.\n",
        "\n",
        "> Afin de simplifier le travail, nous avons choisi de ne pas utiliser de *batchs* dans la fonction qui teste les questions.\n",
        "> Vous n'avez pas à prendre en compte le *warning* qui suggère d'utiliser des *datasets*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c52df54",
      "metadata": {
        "id": "9c52df54"
      },
      "source": [
        "Description des paramètres de génération:\n",
        "(à compléter...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8fde84d7791669a0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.193330Z",
          "start_time": "2025-10-19T23:15:34.188064Z"
        },
        "id": "8fde84d7791669a0"
      },
      "outputs": [],
      "source": [
        "def process_entry(entry, prompt_builder, generator):\n",
        "    prompt = prompt_builder(entry)\n",
        "    generation_output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=10, # limit the length of the generated answer\n",
        "        do_sample=False,    # enable sampling for more varied responses\n",
        "        temperature=0.8,   # control the randomness of the output\n",
        "        top_k=50,          # consider only the top k most likely next tokens\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    # The generated_text includes the prompt, so we need to extract only the answer\n",
        "    generated_text = generation_output[0]['generated_text']\n",
        "    answer = generated_text[len(prompt):].strip()\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4350b250",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.212276Z",
          "start_time": "2025-10-19T23:15:34.205869Z"
        },
        "id": "4350b250"
      },
      "outputs": [],
      "source": [
        "def test_on_questions(prompt_builder, model_path, question_file, out_file_name, output_dir=\"results\"):\n",
        "    entries = load_entries(question_file)\n",
        "    tokenizer, model = load_model(model_path)\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    results = []\n",
        "    for i, entry in enumerate(entries):\n",
        "        answer = process_entry({\"instruction\": entry[\"question\"]}, prompt_builder, generator)\n",
        "        question = entry.get(\"question\", \"\")\n",
        "        expected_answer = entry.get(\"answer\", \"\")\n",
        "        results.append((i, question, answer, expected_answer))\n",
        "    save_answers(results, output_dir, out_file_name, display=True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ba4601",
      "metadata": {
        "id": "e4ba4601"
      },
      "source": [
        "## 3. Préparation des données et des prompts pour l'affinage du modèle\n",
        "\n",
        "Le code suivant prépare les ressources nécessaires pour l'affinage du modèle GPT2 pré-entraîné dans la tâche 3 de ce travail.\n",
        "\n",
        "Les étapes sont :\n",
        "* Télécharger le fichier de données Alpaca, le jeu d'instructions utilisé pour l'affinage du modèle . Afin de limiter le temps d'entraînement, on retient seulement les 5000 premières instructions de ce *dataset*. Vous pouvez modifier ce nombre si vous le souhaitez.\n",
        "* Générer un prompt spécifique à Alpaca.\n",
        "\n",
        "On rend disponible tout ce qui est nécessaire pour ces 2 étapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4a0b0e8835a7572",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:35.426389Z",
          "start_time": "2025-10-19T23:15:34.222036Z"
        },
        "id": "4a0b0e8835a7572"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "alpaca_url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n",
        "\n",
        "def load_or_download_instruct_dataset_file(data_url, file_path, count=-1):\n",
        "    with urllib.request.urlopen(data_url) as response:\n",
        "        raw_data = response.read().decode(\"utf-8\")\n",
        "        data = json.loads(raw_data)\n",
        "    if count > 0 and count <= len(data):\n",
        "        data = data[:count]\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "instructions_fn = \"data/alpaca_data.json\"  # Fichier où sont enregistrées les instructions d'affinage du modèle\n",
        "nb_instructions = 5000  # Ce nombre peut-être modifié\n",
        "load_or_download_instruct_dataset_file(data_url=alpaca_url, count=nb_instructions, file_path=instructions_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27ac4464a8f26ba6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:35.442369Z",
          "start_time": "2025-10-19T23:15:35.437440Z"
        },
        "id": "27ac4464a8f26ba6"
      },
      "outputs": [],
      "source": [
        "def alpaca_build_prompt(ex):\n",
        "    instruction = ex.get(\"instruction\", \"\")\n",
        "    input = ex.get(\"input\", \"\").strip()\n",
        "    header = \"Below is an instruction that describes a task\"\n",
        "    if input:\n",
        "        header += \", paired with an input\"\n",
        "    return (\n",
        "        f\"{header}.\\n\"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "        + (f\"### Input:\\n{input}\\n\\n\" if input else \"\")\n",
        "        + \"### Response:\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb13219",
      "metadata": {
        "id": "deb13219"
      },
      "source": [
        "## 4. Affinage du modèle (à compléter)\n",
        "\n",
        "Complétez le code suivant pour affiner le modèle GPT2 préentraîné et sauvegardé dans la tâche 3 de ce travail.\n",
        "\n",
        "Les étapes à suivre sont de :\n",
        "* Monter en mémoire le modèle pré-entraîné à la tâche 3 et son tokeniseur\n",
        "* Monter le jeu d'instructions pour l'affinage du modèle et créer un *dataset* avec ces données.\n",
        "* Tokeniser ce *dataset* d'instructions\n",
        "* Faire l'entraînement du modèle sur le *dataset* avec la classe ***Trainer*** de Hugging Face\n",
        "* Faire la sauvegarde du nouveau modèle dans un répertoire (voir *model_path*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "15c5593c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:35.743886Z",
          "start_time": "2025-10-19T23:15:35.455611Z"
        },
        "id": "15c5593c"
      },
      "outputs": [],
      "source": [
        "model_name = \"./nlp_tp2_models/gpt2/gpt2-sherlock-lm\" # Répertoire du modèle construit durant la tâche 3\n",
        "tokenizer, model = load_model(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1cdc69e",
      "metadata": {
        "id": "e1cdc69e"
      },
      "source": [
        "Création du *dataset* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fd049187",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:36.478897Z",
          "start_time": "2025-10-19T23:15:35.769817Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cf2015979eae4641bba25cacae3805ca",
            "828df782cb3345e480f6790044075dd0",
            "031d6643be7f4955a9502a7a9708c33a",
            "5113c756ff5d412fac9d5ae9eec5ab31",
            "facf85eae6c34d14a82389b9b8e9a8f1",
            "0e02a4c6e1464f169e376a1571503481",
            "7fe7c2dcfe914a42bf7aafac3bad2ad7",
            "405c17e74720443184a3c025ea8eefd6",
            "460f4ac4efd24518b9bfbeffa4d915d0",
            "a4776ada6ca94808ae6ceaa3466c4f57",
            "2028e5b5caab4b219514488e7979799c"
          ]
        },
        "id": "fd049187",
        "outputId": "18ad2ef0-139c-48e8-ac52-dd72207e90ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf2015979eae4641bba25cacae3805ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "instructions_fn = \"data/alpaca_data.json\"  # Fichier qui contient les instructions d'affinage\n",
        "dataset = Dataset.from_json(instructions_fn)\n",
        "\n",
        "def create_prompt(example):\n",
        "  example[\"text\"] = alpaca_build_prompt(example)\n",
        "  return example\n",
        "\n",
        "txt_dataset = dataset.map(create_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba80b0f8",
      "metadata": {
        "id": "ba80b0f8"
      },
      "source": [
        "Tokénisation du *dataset* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "71d42771",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:40.719083Z",
          "start_time": "2025-10-19T23:15:36.530401Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "d99421fc9bca48a389c9a52f2d301929",
            "98500675ccae477b820d7554a63a22fe",
            "24342aa51ff34db5aab7d57e35cd9ee2",
            "b624538652704344ab995b88d0e0fc4d",
            "1c79abb6f8fd4c68b497d80c2c21a806",
            "23b4f229461b4567972f6aede0f555ed",
            "bbdb966bae5a422b86096e5c13374dd6",
            "a580463ea1d8459aaf1ff00764af625e",
            "fc219f26fbe641478627b1934c3b5bbd",
            "9251dc4816a94ef2a023b3ebdedc0738",
            "d5fd3939a541464291112f3f755e7792"
          ]
        },
        "id": "71d42771",
        "outputId": "777fefb5-c67c-4472-f5d8-f796a0e976f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d99421fc9bca48a389c9a52f2d301929"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task.\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n', 'input_ids': [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 198, 16594, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 23318, 1115, 9040, 329, 10589, 5448, 13, 198, 198, 21017, 18261, 25, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "def tokenize_example(example):\n",
        "    return tokenizer(example[\"text\"], max_length=max_length, truncation=True)\n",
        "\n",
        "tokenized_dataset = txt_dataset.map(tokenize_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f738415a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:36.506618Z",
          "start_time": "2025-10-19T23:15:36.500252Z"
        },
        "id": "f738415a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/checkpoints\",\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    num_train_epochs=6,\n",
        "\n",
        "    # 2. BATCH SIZE: Small dataset = requires stability.\n",
        "    # We use gradient accumulation to simulate a larger batch size (e.g., 32)\n",
        "    # while keeping memory usage low.\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4, # Effective batch size = 4 * 8 = 32\n",
        "\n",
        "    # 3. LEARNING RATE: The most critical part.\n",
        "    # Standard is 5e-5. Since your data is small, sticking to the lower end\n",
        "    # prevents destroying the pre-trained knowledge.\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # 4. SCHEDULER: vital for small data\n",
        "    warmup_steps=50, # Warm up quickly (roughly 1 epoch worth of steps)\n",
        "    lr_scheduler_type=\"cosine\", # Smooth decay is better than linear for language\n",
        "\n",
        "    # 5. LOGGING\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    fp16=True, # Use mixed precision if on GPU (much faster)\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b23aca5c",
      "metadata": {
        "id": "b23aca5c"
      },
      "source": [
        "Ajouter dans ces cellules tout le code dont vous avez besoin pour faire l'affinage du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2732703a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:42.892925Z",
          "start_time": "2025-10-19T23:15:40.722063Z"
        },
        "id": "2732703a"
      },
      "outputs": [],
      "source": [
        "instruct_model_path = \"./nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct\" # Répertoire où sauvegarder le nouveau modèle et le tokenizer\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3e9f67bf566a5b28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:22:22.666862Z",
          "start_time": "2025-10-19T23:15:42.951795Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3e9f67bf566a5b28",
        "outputId": "50936c80-7530-44d8-e1a4-1196614d8e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [471/471 06:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.270200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.229100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.009400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.217700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.051500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.919100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.965500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.921300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.911200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.925200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.911900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.907700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.907900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.839700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.839300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.834400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.823200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.818300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.809200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.820400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.816000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.813600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.807700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.805900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.810900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.807000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.828100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.763200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.755900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.819500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.770400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.802900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.749800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.755100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.806200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.775600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.766500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.754800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.784600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.770900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.735300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=471, training_loss=0.9893286910279795, metrics={'train_runtime': 387.8002, 'train_samples_per_second': 38.68, 'train_steps_per_second': 1.215, 'total_flos': 2171128092426240.0, 'train_loss': 0.9893286910279795, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993e9d7f",
      "metadata": {
        "id": "993e9d7f"
      },
      "source": [
        "Pour conclure cette section, sauvegardez le nouveau modèle et le tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7e551d44",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:22:25.212503Z",
          "start_time": "2025-10-19T23:22:22.748102Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e551d44",
        "outputId": "cd3b8c11-0f0e-41ae-9ae4-6217a4a33678"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/tokenizer_config.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/special_tokens_map.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/vocab.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/merges.txt',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/added_tokens.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "trainer.save_model(instruct_model_path)\n",
        "tokenizer.save_pretrained(instruct_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc459e7a",
      "metadata": {
        "id": "dc459e7a"
      },
      "source": [
        "## 5. Génération des réponses pour les questions de Sherlock Holmes\n",
        "\n",
        "Exécutez la cellule suivante pour générer, avec le modèle affiné, les réponses aux questions de test.\n",
        "Le temps d’exécution devrait se situer entre **5 et 10 minutes** si vous utilisez **Google Colab** avec un GPU.\n",
        "\n",
        "Note : N'oubliez pas d'ajouter le fichier de réponses générées par le modèle (voir *out_file_name*) dans votre remise du travail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "_ntrtd3jWnhs",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-10-19T23:22:25.238643Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ntrtd3jWnhs",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "e5c6012d-699f-4095-eb23-1159ed382b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q0: Where do Sherlock Holmes and Dr. Watson live?\n",
            "A: Sherlock Holmes lives in London, while Dr.\n",
            "Expected:221B Baker Street, London.\n",
            "------------------------------------------------------------\n",
            "Q1: Who is Sherlock Holmes' loyal friend and chronicler?\n",
            "A: John Watson\n",
            "\n",
            "### Instruction:\n",
            "What is\n",
            "Expected:Dr. John H. Watson.\n",
            "------------------------------------------------------------\n",
            "Q2: Who is considered 'The Woman' by Sherlock Holmes?\n",
            "A: Sherlock Holmes\n",
            "\n",
            "### Instruction:\n",
            "What\n",
            "Expected:Irene Adler.\n",
            "------------------------------------------------------------\n",
            "Q3: Which story features the Red-Headed League?\n",
            "A: The Red-Headed League is a fictional organization\n",
            "Expected:The Adventure of the Red-Headed League.\n",
            "------------------------------------------------------------\n",
            "Q4: What is the primary occupation of Sherlock Holmes?\n",
            "A: Sherlock Holmes is a British detective who is best\n",
            "Expected:Consulting detective.\n",
            "------------------------------------------------------------\n",
            "Q5: Who is Sherlock Holmes' arch-nemesis?\n",
            "A: John Watson\n",
            "\n",
            "### Instruction:\n",
            "What is\n",
            "Expected:Professor James Moriarty.\n",
            "------------------------------------------------------------\n",
            "Q6: What musical instrument does Sherlock Holmes play?\n",
            "A: The violin\n",
            "\n",
            "### Instruction:\n",
            "What is\n",
            "Expected:The violin.\n",
            "------------------------------------------------------------\n",
            "Q7: What is the name of Sherlock Holmes' elder brother?\n",
            "A: John\n",
            "\n",
            "### Instruction:\n",
            "What is the\n",
            "Expected:Mycroft Holmes.\n",
            "------------------------------------------------------------\n",
            "Q8: What is the blue gemstone found inside a Christmas goose called?\n",
            "A: The blue gemstone is a rare and beautiful gem\n",
            "Expected:The Blue Carbuncle.\n",
            "------------------------------------------------------------\n",
            "Q9: What residue does Holmes often analyze to identify smokers?\n",
            "A: The answer is a mixture of nicotine and carbon mon\n",
            "Expected:Tobacco ash.\n",
            "------------------------------------------------------------\n",
            "Q10: What tactic besides observation does Holmes often use to gather information?\n",
            "A: He often uses the use of deduction to make deductions\n",
            "Expected:Disguise.\n",
            "------------------------------------------------------------\n",
            "Q11: What was the profession of Dr. John Watson?\n",
            "A: He was a British scientist who invented the theory of\n",
            "Expected:Doctor.\n",
            "------------------------------------------------------------\n",
            "Q12: What kind of marks on the ground does Holmes often study to track people?\n",
            "A: The marks are usually small, irregular shapes that are\n",
            "Expected:Footprints.\n",
            "------------------------------------------------------------\n",
            "Q13: Who is the landlady of 221B Baker Street?\n",
            "A: The landlady is a woman who lives in\n",
            "Expected:Mrs. Hudson.\n",
            "------------------------------------------------------------\n",
            "Q14: What substance did Sherlock Holmes sometimes use to stimulate his mind?\n",
            "A: He used to use a lot of different substances to\n",
            "Expected:Cocaine.\n",
            "------------------------------------------------------------\n",
            "Q15: Who is the Scotland Yard detective that often consults Holmes?\n",
            "A: No response.\n",
            "\n",
            "### Instruction:\n",
            "What\n",
            "Expected:Inspector Lestrade (Gregson or Bradstreet also acceptable).\n",
            "------------------------------------------------------------\n",
            "Q16: What warning arrives as envelopes containing dried orange seeds?\n",
            "A: No response\n",
            "\n",
            "### Instruction:\n",
            "What is\n",
            "Expected:Five orange pips.\n",
            "------------------------------------------------------------\n",
            "Q17: What is the bog near Baskerville Hall called?\n",
            "A: The bog is a large, flat area of land\n",
            "Expected:The Grimpen Mire.\n",
            "------------------------------------------------------------\n",
            "Q18: Which London newspaper does Holmes frequently read?\n",
            "A: The Daily Telegraph\n",
            "\n",
            "### Instruction:\n",
            "What\n",
            "Expected:The Times.\n",
            "------------------------------------------------------------\n",
            "Q19: What kind of drawings form the code that Holmes deciphers on walls and paper?\n",
            "A: A drawing of a cat, a drawing of a\n",
            "Expected:Dancing stick figures.\n",
            "------------------------------------------------------------\n",
            "Q20: What kind of jewel is set in the damaged coronet handled by a banker?\n",
            "A: The jewel is a set of diamonds with a small\n",
            "Expected:Beryls.\n",
            "------------------------------------------------------------\n",
            "Q21: What combat sport is Holmes proficient in?\n",
            "A: He is a professional boxer and has won numerous world\n",
            "Expected:Boxing.\n",
            "------------------------------------------------------------\n",
            "Q22: What does Sherlock Holmes keep in his Persian slipper?\n",
            "A: It is a small, waterproof, waterproof slipper\n",
            "Expected:Pipe tobacco.\n",
            "------------------------------------------------------------\n",
            "Q23: In which room at 221B do most client interviews happen?\n",
            "A: The interview room is located in the main office.\n",
            "Expected:The sitting-room.\n",
            "------------------------------------------------------------\n",
            "Q24: What important document goes missing from the Foreign Office in one case?\n",
            "A: The missing document is a document on the use of\n",
            "Expected:A secret naval treaty.\n",
            "------------------------------------------------------------\n",
            "Q25: What hot drink is often served at Baker Street?\n",
            "A: The popular drink is a iced tea with whipped\n",
            "Expected:Tea.\n",
            "------------------------------------------------------------\n",
            "Q26: What object ties Irene Adler to a scandal with a European king?\n",
            "A: The king of France is accused of having a secret\n",
            "Expected:A compromising photograph.\n",
            "------------------------------------------------------------\n",
            "Q27: What weapon does Dr. Watson often carry?\n",
            "A: A revolver\n",
            "\n",
            "### Instruction:\n",
            "What is\n",
            "Expected:A revolver.\n",
            "------------------------------------------------------------\n",
            "Q28: In which country did Dr. Watson serve as an army doctor?\n",
            "A: The United Kingdom\n",
            "\n",
            "### Instruction:\n",
            "What\n",
            "Expected:Afghanistan.\n",
            "------------------------------------------------------------\n",
            "Q29: What is the missing racehorse’s name in the Dartmoor case?\n",
            "A: The missing horse is named Dartmoor.\n",
            "Expected:Silver Blaze.\n",
            "------------------------------------------------------------\n",
            "Q30: What London vehicle do Holmes and Watson frequently hire for short trips?\n",
            "A: A black Ford Focus.\n",
            "\n",
            "### Instruction:\n",
            "Expected:A hansom cab.\n",
            "------------------------------------------------------------\n",
            "Q31: In which English county do Holmes and Watson investigate a deadly household powder?\n",
            "A: The answer is:\n",
            "\n",
            "### Instruction:\n",
            "Expected:Cornwall.\n",
            "------------------------------------------------------------\n",
            "Q32: Where does Holmes retire?\n",
            "A: He is a retired professor of physics at the University\n",
            "Expected:Sussex Downs.\n",
            "------------------------------------------------------------\n",
            "Q33: What does Watson call Holmes’ method of reasoning?\n",
            "A: The Watson method of reasoning is a method of reasoning\n",
            "Expected:“The science of deduction and analysis.”\n",
            "------------------------------------------------------------\n",
            "Q34: What is the name of the street‑boy network Holmes employs?\n",
            "A: The name is \"Holmes\" and the network\n",
            "Expected:The Baker Street Irregulars.\n",
            "------------------------------------------------------------\n",
            "Q35: What London club is Mycroft Holmes most associated with?\n",
            "A: The London club is the London based sports club.\n",
            "Expected:The Diogenes Club.\n",
            "------------------------------------------------------------\n",
            "Q36: What is the name of the ancient document recited by Reginald Musgrave?\n",
            "A: The Declaration of Independence\n",
            "\n",
            "### Instruction:\n",
            "Expected:The Musgrave Ritual.\n",
            "------------------------------------------------------------\n",
            "Q37: What injury does Victor Hatherley suffer during his adventure?\n",
            "A: He is injured in a fall from a cliff.\n",
            "Expected:Loss of a thumb.\n",
            "------------------------------------------------------------\n",
            "Q38: On what moor do the Baskerville events occur?\n",
            "A: The Baskerville Adventure is a series of adventures\n",
            "Expected:Dartmoor.\n",
            "------------------------------------------------------------\n",
            "Q39: What substance is used to make an animal appear ghostly?\n",
            "A: A substance used to make an animal appear ghostly\n",
            "Expected:Phosphorus.\n",
            "------------------------------------------------------------\n",
            "Q40: In which county is Dartmoor located?\n",
            "A: Dartmoor, England\n",
            "\n",
            "### Instruction\n",
            "Expected:Devonshire.\n",
            "------------------------------------------------------------\n",
            "Q41: What animal do Holmes and Watson sometimes use to track a scent?\n",
            "A: The dog\n",
            "\n",
            "### Instruction:\n",
            "What is\n",
            "Expected:A dog.\n",
            "------------------------------------------------------------\n",
            "Q42: What scientific field is Holmes notably skilled in?\n",
            "A: The field of artificial intelligence.\n",
            "\n",
            "### Instruction\n",
            "Expected:Chemistry.\n",
            "------------------------------------------------------------\n",
            "Q43: Which police force does Holmes frequently assist?\n",
            "A: The police force is the largest in the city.\n",
            "Expected:Scotland Yard.\n",
            "------------------------------------------------------------\n",
            "Q44: What railway timetable does Holmes often consult?\n",
            "A: He often uses the London Underground to travel to and\n",
            "Expected:Bradshaw's.\n",
            "------------------------------------------------------------\n",
            "Q45: What supernatural creature is suspected in a case involving a South American family?\n",
            "A: The family is from the Amazon rainforest.\n",
            "Expected:A vampire.\n",
            "------------------------------------------------------------\n",
            "Q46: What quick message service does Holmes often use?\n",
            "A: I use a variety of services, including email,\n",
            "Expected:Telegrams.\n",
            "------------------------------------------------------------\n",
            "Q47: What handheld tool does Holmes use to inspect tiny clues?\n",
            "A: He uses a magnifying glass to look at the\n",
            "Expected:A magnifying glass.\n",
            "------------------------------------------------------------\n",
            "Q48: In which city do most of Holmes's cases take place?\n",
            "A: London\n",
            "\n",
            "### Instruction:\n",
            "In which city\n",
            "Expected:London.\n",
            "------------------------------------------------------------\n",
            "Q49: After retirement, what hobby does Holmes pursue?\n",
            "A: I am a professional musician.\n",
            "\n",
            "### Instruction\n",
            "Expected:Beekeeping.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "questions_fn = \"data/questions_sherlock.json\"\n",
        "out_file_name = \"instruct_gpt2_answers.txt\"\n",
        "\n",
        "results = test_on_questions(prompt_builder=alpaca_build_prompt, model_path=instruct_model_path, question_file=questions_fn, out_file_name=out_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df7d2c0",
      "metadata": {
        "id": "4df7d2c0"
      },
      "source": [
        " ## 6. Analyse des résultats\n",
        "\n",
        " ### 6.1 Évaluation quantitative (à compléter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "8d105903",
      "metadata": {
        "id": "8d105903"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c884b836",
      "metadata": {
        "id": "c884b836"
      },
      "outputs": [],
      "source": [
        "def evaluate_f1(ground_truth, prediction):\n",
        "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en commun et estime précision, rappel et F1.\"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "65fe1607",
      "metadata": {
        "id": "65fe1607"
      },
      "outputs": [],
      "source": [
        "def evaluation_generation(results):\n",
        "    eval = {\"precision\":0, \"recall\":0, \"f1\":0}\n",
        "    for i, question, answer, expected_answer in results:\n",
        "        precision, recall, f1 = evaluate_f1(expected_answer, answer)\n",
        "        eval[\"precision\"] += precision\n",
        "        eval[\"recall\"] += recall\n",
        "        eval[\"f1\"] += f1\n",
        "\n",
        "    eval[\"precision\"] /= len(results)\n",
        "    eval[\"recall\"] /= len(results)\n",
        "    eval[\"f1\"] /= len(results)\n",
        "\n",
        "    return eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6dd0f919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd0f919",
        "outputId": "c1dfff00-b4a3-4449-b438-2677b95f4d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.05714285714285714, 'recall': 0.17833333333333334, 'f1': 0.08171572871572871}\n"
          ]
        }
      ],
      "source": [
        "eval = evaluation_generation(results)\n",
        "print(eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521bcbe2",
      "metadata": {
        "id": "521bcbe2"
      },
      "source": [
        "**Question :** Que pensez-vous de cette évaluation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f20fe54",
      "metadata": {
        "id": "8f20fe54"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "09d00e33",
      "metadata": {
        "id": "09d00e33"
      },
      "source": [
        "### 6.2 Analyse qualitative (à faire)\n",
        "\n",
        "Faites l'analyse des réponses de ce modèle. Présentez vos observations par rapport aux réponses obtenus des modèles des tâches 2 et 3.\n",
        "\n",
        "Expliquez ce que vous retenez des 3 dernières tâches sur le pré-entraînement et le post-entraînement du modèle GPT-2.\n",
        "\n",
        "Vous pouvez ajouter des cellules au besoin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10bab549",
      "metadata": {
        "id": "10bab549"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf2015979eae4641bba25cacae3805ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_828df782cb3345e480f6790044075dd0",
              "IPY_MODEL_031d6643be7f4955a9502a7a9708c33a",
              "IPY_MODEL_5113c756ff5d412fac9d5ae9eec5ab31"
            ],
            "layout": "IPY_MODEL_facf85eae6c34d14a82389b9b8e9a8f1"
          }
        },
        "828df782cb3345e480f6790044075dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e02a4c6e1464f169e376a1571503481",
            "placeholder": "​",
            "style": "IPY_MODEL_7fe7c2dcfe914a42bf7aafac3bad2ad7",
            "value": "Map: 100%"
          }
        },
        "031d6643be7f4955a9502a7a9708c33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_405c17e74720443184a3c025ea8eefd6",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_460f4ac4efd24518b9bfbeffa4d915d0",
            "value": 5000
          }
        },
        "5113c756ff5d412fac9d5ae9eec5ab31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4776ada6ca94808ae6ceaa3466c4f57",
            "placeholder": "​",
            "style": "IPY_MODEL_2028e5b5caab4b219514488e7979799c",
            "value": " 5000/5000 [00:00&lt;00:00, 16104.42 examples/s]"
          }
        },
        "facf85eae6c34d14a82389b9b8e9a8f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e02a4c6e1464f169e376a1571503481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe7c2dcfe914a42bf7aafac3bad2ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "405c17e74720443184a3c025ea8eefd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460f4ac4efd24518b9bfbeffa4d915d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4776ada6ca94808ae6ceaa3466c4f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2028e5b5caab4b219514488e7979799c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d99421fc9bca48a389c9a52f2d301929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98500675ccae477b820d7554a63a22fe",
              "IPY_MODEL_24342aa51ff34db5aab7d57e35cd9ee2",
              "IPY_MODEL_b624538652704344ab995b88d0e0fc4d"
            ],
            "layout": "IPY_MODEL_1c79abb6f8fd4c68b497d80c2c21a806"
          }
        },
        "98500675ccae477b820d7554a63a22fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b4f229461b4567972f6aede0f555ed",
            "placeholder": "​",
            "style": "IPY_MODEL_bbdb966bae5a422b86096e5c13374dd6",
            "value": "Map: 100%"
          }
        },
        "24342aa51ff34db5aab7d57e35cd9ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a580463ea1d8459aaf1ff00764af625e",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc219f26fbe641478627b1934c3b5bbd",
            "value": 5000
          }
        },
        "b624538652704344ab995b88d0e0fc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9251dc4816a94ef2a023b3ebdedc0738",
            "placeholder": "​",
            "style": "IPY_MODEL_d5fd3939a541464291112f3f755e7792",
            "value": " 5000/5000 [00:01&lt;00:00, 3051.73 examples/s]"
          }
        },
        "1c79abb6f8fd4c68b497d80c2c21a806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b4f229461b4567972f6aede0f555ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbdb966bae5a422b86096e5c13374dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a580463ea1d8459aaf1ff00764af625e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc219f26fbe641478627b1934c3b5bbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9251dc4816a94ef2a023b3ebdedc0738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fd3939a541464291112f3f755e7792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}