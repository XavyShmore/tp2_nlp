{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d53d4c2",
      "metadata": {
        "id": "4d53d4c2"
      },
      "source": [
        "# Tâche 4 : Question-réponse avec affinage par instructions du modèle GPT‑2 pré-entraîné sur Sherlock Holmes\n",
        "\n",
        "**Objectifs**\n",
        "\n",
        "Évaluer la qualité des réponses d’un modèle de langage **pré‑entraîné** (celui de la tâche 3) et affiner par instructions (cette tâche).\n",
        "Dans ce *notebook*, vous faites le post-entraînement du modèle avec des instructions générales indiquant au modèle comment accomplir des tâches simples. Comme pour les tâches 2 et 3, la démarche de test est de construire un *prompt*, de générer des réponses, et d'évaluer qualitativement la pertinence des résultats. Plusieurs de ces fonctions sont rendues disponibles.\n",
        "\n",
        "**Objectifs d’apprentissage**\n",
        "1. Faire le post-entraînement d'un modèle pré‑entraîné avec l'affinage par instructions (*instruction tuning*).\n",
        "2. Comprendre et expliquer les **limites et apports de l'affinage par instructions** d'un modèle.\n",
        "\n",
        "Tout comme dans les tâches 2 et 3, les **questions** pour évaluer le modèle vous sont fournies. Le fichier d'**instructions** pour l'affinage du modèle est également fourni. Vous devez comprendre le format des questions chargées en mémoire. Il est également important de prendre connaissance de la nature des instructions utilisées pour l'affinage.\n",
        "\n",
        "> Il est recommandé de faire ce travail pratique en utilisant une carte graphique GPU compatible avec HuggingFace/Pytorch.\n",
        "> Si votre machine n’en possède pas, vous pouvez utiliser **Google Colab** pour exécuter le *notebook* dans le cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81af0ea3655c4578",
      "metadata": {
        "id": "81af0ea3655c4578"
      },
      "source": [
        "Si nécessaire, installer les *packages* suivant. Si vous exécutez sur Code Colab, ces *packages* devraient déjà être installés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "P1jvDsOV7WVW",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:22.883417Z",
          "start_time": "2025-10-19T23:15:22.872259Z"
        },
        "id": "P1jvDsOV7WVW"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install accelerate\n",
        "#!pip install 'transformers[torch]'\n",
        "#!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4d460d1d84efc873",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:23.130682Z",
          "start_time": "2025-10-19T23:15:23.125998Z"
        },
        "id": "4d460d1d84efc873"
      },
      "outputs": [],
      "source": [
        "batch_size = 5 # il est possible d'ajuster la taille de batch. Les valeurs actuelles utilisent environ 10 Gb\n",
        "max_length = 256 # on réduit le contexte pour sauver du temps, nos exemples ne nécesside pas un plus grand contexte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "660a299e09c87174",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.145468Z",
          "start_time": "2025-10-19T23:15:23.155231Z"
        },
        "id": "660a299e09c87174"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import pipeline, Trainer\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "#Mount a google drive folder to save things\n",
        "if IN_COLAB:\n",
        "  drive.mount('/content/drive')\n",
        "  folders_to_mount = [\"nlp_tp2_models\", \"results\"]\n",
        "  for folder in folders_to_mount:\n",
        "    source = f'/content/drive/MyDrive/uni/nlp/{folder}'\n",
        "    shortcut = f'/content/{folder}'\n",
        "    print(f\"Mounting {source} to {shortcut}\")\n",
        "    os.symlink(source, shortcut)\n",
        "\n",
        "repo_url = \"https://github.com/XavyShmore/tp2_nlp.git\"\n",
        "if IN_COLAB:\n",
        "  !git clone {repo_url}\n",
        "  !cp -r ./tp2_nlp/data .\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec798R5cXRSW",
        "outputId": "26212276-53a9-406d-80aa-b510433a04db"
      },
      "id": "ec798R5cXRSW",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounting /content/drive/MyDrive/uni/nlp/nlp_tp2_models to /content/nlp_tp2_models\n",
            "Mounting /content/drive/MyDrive/uni/nlp/results to /content/results\n",
            "Cloning into 'tp2_nlp'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 49 (delta 26), reused 10 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 2.48 MiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b911ebeb",
      "metadata": {
        "id": "b911ebeb"
      },
      "source": [
        "## 1. Chargement du modèle Hugging Face et du tokenizer (à compléter)\n",
        "\n",
        "Complétez le corps de la fonction `load_model(model_path)` afin qu’elle :\n",
        "\n",
        "- charge le **tokenizer** et le **modèle** Hugging Face à partir du chemin `model_path`.\n",
        "- **retourne** le tokenizer comme **première valeur de retour** et le modèle comme **seconde valeur de retour**.\n",
        "\n",
        "On ajoute également des fonctions pour monter les questions en mémoire et pour sauvegarder les réponses dans un fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f9254d41",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.160378Z",
          "start_time": "2025-10-19T23:15:34.155815Z"
        },
        "id": "f9254d41"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def load_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4c56a1e65ebeb313",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.177800Z",
          "start_time": "2025-10-19T23:15:34.171427Z"
        },
        "id": "4c56a1e65ebeb313"
      },
      "outputs": [],
      "source": [
        "def load_entries(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(f\"Question file must contain a list of objects. Got: {type(data)}\")\n",
        "    return data\n",
        "\n",
        "def save_answers(questions_answers, output_dir, out_file_name, display=True):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(os.path.join(output_dir, out_file_name), \"w\", encoding=\"utf-8\") as out:\n",
        "        for index, question, answer, expected_answer in questions_answers:\n",
        "            out.write(f\"Q: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\\n\")\n",
        "            if display:\n",
        "                print(f\"Q{index}: {question}\\nA: {answer}\\nExpected:{expected_answer}\\n{'-' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c168134c",
      "metadata": {
        "id": "c168134c"
      },
      "source": [
        "## 2. Fonctions de test question-réponse  (à compléter)\n",
        "\n",
        "La fonction **test_on_questions** est utilisée pour parcourir **toutes les entrées** du fichier de questions afin de produire des réponses générées par le modèle.\n",
        "\n",
        "La génération d'une réponse à une question implique les étapes suivantes (fonction **process_entry** à compléter) :\n",
        "* Construire un prompt à l’aide de la fonction **alpaca_build_prompt** (rendu disponible dans la prochaine section)\n",
        "* Utiliser le modèle (via un pipeline de génération de texte) pour générer une réponse à une question\n",
        "* Retourner la réponse générée par le modèle.  \n",
        "\n",
        "Points importants à souligner:\n",
        "* La fonction *process_entry* doit retourner uniquement la réponse générée par le modèle (sans le prompt).\n",
        "* Il est de votre responsabililté de choisir **les paramètres** du générateur (max_new_tokens, do_sample, temperature, top_k ou top_p). Décrivez ceux que vous avez retenus.\n",
        "\n",
        "> Afin de simplifier le travail, nous avons choisi de ne pas utiliser de *batchs* dans la fonction qui teste les questions.\n",
        "> Vous n'avez pas à prendre en compte le *warning* qui suggère d'utiliser des *datasets*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c52df54",
      "metadata": {
        "id": "9c52df54"
      },
      "source": [
        "Description des paramètres de génération:\n",
        "(à compléter...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "8fde84d7791669a0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.193330Z",
          "start_time": "2025-10-19T23:15:34.188064Z"
        },
        "id": "8fde84d7791669a0"
      },
      "outputs": [],
      "source": [
        "def process_entry(entry, prompt_builder, generator):\n",
        "    prompt = prompt_builder(entry)\n",
        "    generation_output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=20, # limit the length of the generated answer\n",
        "        do_sample=False,    # enable sampling for more varied responses\n",
        "        temperature=0.4,   # control the randomness of the output\n",
        "        top_k=50,          # consider only the top k most likely next tokens\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    # The generated_text includes the prompt, so we need to extract only the answer\n",
        "    generated_text = generation_output[0]['generated_text']\n",
        "    answer = generated_text[len(prompt):].strip()\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ajouter cela avant la question afin pouvoir bien comparer avec les modèles précédent qui avaient cela dans leur prompt.\n",
        "pre_prompt = \"Answer the question \""
      ],
      "metadata": {
        "id": "WsSgydj43XiP"
      },
      "id": "WsSgydj43XiP",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "4350b250",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:34.212276Z",
          "start_time": "2025-10-19T23:15:34.205869Z"
        },
        "id": "4350b250"
      },
      "outputs": [],
      "source": [
        "def test_on_questions(prompt_builder, model_path, question_file, out_file_name, output_dir=\"results\"):\n",
        "    entries = load_entries(question_file)\n",
        "    tokenizer, model = load_model(model_path)\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    results = []\n",
        "    for i, entry in enumerate(entries):\n",
        "        answer = process_entry({\"instruction\": pre_prompt + entry[\"question\"]}, prompt_builder, generator)\n",
        "        question = entry.get(\"question\", \"\")\n",
        "        expected_answer = entry.get(\"answer\", \"\")\n",
        "        results.append((i, question, answer, expected_answer))\n",
        "    save_answers(results, output_dir, out_file_name, display=True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ba4601",
      "metadata": {
        "id": "e4ba4601"
      },
      "source": [
        "## 3. Préparation des données et des prompts pour l'affinage du modèle\n",
        "\n",
        "Le code suivant prépare les ressources nécessaires pour l'affinage du modèle GPT2 pré-entraîné dans la tâche 3 de ce travail.\n",
        "\n",
        "Les étapes sont :\n",
        "* Télécharger le fichier de données Alpaca, le jeu d'instructions utilisé pour l'affinage du modèle . Afin de limiter le temps d'entraînement, on retient seulement les 5000 premières instructions de ce *dataset*. Vous pouvez modifier ce nombre si vous le souhaitez.\n",
        "* Générer un prompt spécifique à Alpaca.\n",
        "\n",
        "On rend disponible tout ce qui est nécessaire pour ces 2 étapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4a0b0e8835a7572",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:35.426389Z",
          "start_time": "2025-10-19T23:15:34.222036Z"
        },
        "id": "4a0b0e8835a7572"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "alpaca_url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n",
        "\n",
        "def load_or_download_instruct_dataset_file(data_url, file_path, count=-1):\n",
        "    with urllib.request.urlopen(data_url) as response:\n",
        "        raw_data = response.read().decode(\"utf-8\")\n",
        "        data = json.loads(raw_data)\n",
        "    if count > 0 and count <= len(data):\n",
        "        data = data[:count]\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "instructions_fn = \"data/alpaca_data.json\"  # Fichier où sont enregistrées les instructions d'affinage du modèle\n",
        "nb_instructions = 5000  # Ce nombre peut-être modifié\n",
        "load_or_download_instruct_dataset_file(data_url=alpaca_url, count=nb_instructions, file_path=instructions_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27ac4464a8f26ba6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:35.442369Z",
          "start_time": "2025-10-19T23:15:35.437440Z"
        },
        "id": "27ac4464a8f26ba6"
      },
      "outputs": [],
      "source": [
        "def alpaca_build_prompt(ex):\n",
        "    instruction = ex.get(\"instruction\", \"\")\n",
        "    input = ex.get(\"input\", \"\").strip()\n",
        "    output = ex.get(\"output\", \"\").strip()\n",
        "    header = \"Below is an instruction that describes a task\"\n",
        "    if input:\n",
        "        header += \", paired with an input\"\n",
        "    return (\n",
        "        f\"{header}.\\n\"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "        + (f\"### Input:\\n{input}\\n\\n\" if input else \"\")\n",
        "        + f\"### Response:\\n{output}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb13219",
      "metadata": {
        "id": "deb13219"
      },
      "source": [
        "## 4. Affinage du modèle (à compléter)\n",
        "\n",
        "Complétez le code suivant pour affiner le modèle GPT2 préentraîné et sauvegardé dans la tâche 3 de ce travail.\n",
        "\n",
        "Les étapes à suivre sont de :\n",
        "* Monter en mémoire le modèle pré-entraîné à la tâche 3 et son tokeniseur\n",
        "* Monter le jeu d'instructions pour l'affinage du modèle et créer un *dataset* avec ces données.\n",
        "* Tokeniser ce *dataset* d'instructions\n",
        "* Faire l'entraînement du modèle sur le *dataset* avec la classe ***Trainer*** de Hugging Face\n",
        "* Faire la sauvegarde du nouveau modèle dans un répertoire (voir *model_path*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "15c5593c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:35.743886Z",
          "start_time": "2025-10-19T23:15:35.455611Z"
        },
        "id": "15c5593c"
      },
      "outputs": [],
      "source": [
        "model_name = \"./nlp_tp2_models/gpt2/gpt2-sherlock-lm\" # Répertoire du modèle construit durant la tâche 3\n",
        "tokenizer, model = load_model(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1cdc69e",
      "metadata": {
        "id": "e1cdc69e"
      },
      "source": [
        "Création du *dataset* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "fd049187",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:36.478897Z",
          "start_time": "2025-10-19T23:15:35.769817Z"
        },
        "id": "fd049187"
      },
      "outputs": [],
      "source": [
        "instructions_fn = \"data/alpaca_data.json\"  # Fichier qui contient les instructions d'affinage\n",
        "dataset = Dataset.from_json(instructions_fn)\n",
        "\n",
        "def create_prompt(example):\n",
        "  example[\"text\"] = alpaca_build_prompt(example)\n",
        "  return example\n",
        "\n",
        "txt_dataset = dataset.map(create_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba80b0f8",
      "metadata": {
        "id": "ba80b0f8"
      },
      "source": [
        "Tokénisation du *dataset* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "71d42771",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:40.719083Z",
          "start_time": "2025-10-19T23:15:36.530401Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d42771",
        "outputId": "8d011c62-876b-4049-c4b6-477fd67fd503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'text': 'Below is an instruction that describes a task.\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'input_ids': [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 198, 16594, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 24564, 4892, 262, 4645, 286, 281, 22037, 13, 198, 198, 21017, 18261, 25, 198, 2025, 22037, 318, 925, 510, 286, 257, 29984, 11, 543, 4909, 1237, 684, 290, 22190, 12212, 11, 11191, 416, 28722, 326, 3067, 287, 37015, 1088, 262, 29984, 13, 383, 1237, 684, 290, 22190, 12212, 423, 257, 3967, 3877, 11, 981, 262, 28722, 423, 257, 4633, 3877, 11, 7186, 287, 281, 4045, 8500, 22037, 13, 383, 1271, 286, 1123, 18758, 15947, 262, 17226, 1271, 290, 262, 2099, 286, 22037, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "def tokenize_example(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_dataset = txt_dataset.map(tokenize_example)\n",
        "print(tokenized_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "f738415a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:36.506618Z",
          "start_time": "2025-10-19T23:15:36.500252Z"
        },
        "id": "f738415a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/checkpoints\",\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    num_train_epochs=1,\n",
        "\n",
        "    # 2. BATCH SIZE: Small dataset = requires stability.\n",
        "    # We use gradient accumulation to simulate a larger batch size (e.g., 32)\n",
        "    # while keeping memory usage low.\n",
        "    per_device_train_batch_size=12,\n",
        "    gradient_accumulation_steps=3, # Effective batch size = 4 * 8 = 32\n",
        "\n",
        "    # 3. LEARNING RATE: The most critical part.\n",
        "    # Standard is 5e-5. Since your data is small, sticking to the lower end\n",
        "    # prevents destroying the pre-trained knowledge.\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # 4. SCHEDULER: vital for small data\n",
        "    warmup_steps=50, # Warm up quickly (roughly 1 epoch worth of steps)\n",
        "    lr_scheduler_type=\"cosine\", # Smooth decay is better than linear for language\n",
        "\n",
        "    # 5. LOGGING\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    fp16=True, # Use mixed precision if on GPU (much faster)\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b23aca5c",
      "metadata": {
        "id": "b23aca5c"
      },
      "source": [
        "Ajouter dans ces cellules tout le code dont vous avez besoin pour faire l'affinage du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "2732703a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:15:42.892925Z",
          "start_time": "2025-10-19T23:15:40.722063Z"
        },
        "id": "2732703a"
      },
      "outputs": [],
      "source": [
        "instruct_model_path = \"./nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct\" # Répertoire où sauvegarder le nouveau modèle et le tokenizer\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3e9f67bf566a5b28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:22:22.666862Z",
          "start_time": "2025-10-19T23:15:42.951795Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "3e9f67bf566a5b28",
        "outputId": "d672e7ea-842f-4dea-fcc6-beb1a4e28c9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [139/139 03:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.477000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.622500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.083500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.761200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.541200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.552200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.494300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.510400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.508300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.466400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=139, training_loss=1.9138462286201312, metrics={'train_runtime': 204.6475, 'train_samples_per_second': 24.432, 'train_steps_per_second': 0.679, 'total_flos': 2028267805900800.0, 'train_loss': 1.9138462286201312, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993e9d7f",
      "metadata": {
        "id": "993e9d7f"
      },
      "source": [
        "Pour conclure cette section, sauvegardez le nouveau modèle et le tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "7e551d44",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-19T23:22:25.212503Z",
          "start_time": "2025-10-19T23:22:22.748102Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e551d44",
        "outputId": "20534acf-59bf-4bce-cc64-2ec7645b28cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/tokenizer_config.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/special_tokens_map.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/vocab.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/merges.txt',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/added_tokens.json',\n",
              " './nlp_tp2_models/gpt2/gpt2-sherlock-lm-instruct/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "trainer.save_model(instruct_model_path)\n",
        "tokenizer.save_pretrained(instruct_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc459e7a",
      "metadata": {
        "id": "dc459e7a"
      },
      "source": [
        "## 5. Génération des réponses pour les questions de Sherlock Holmes\n",
        "\n",
        "Exécutez la cellule suivante pour générer, avec le modèle affiné, les réponses aux questions de test.\n",
        "Le temps d’exécution devrait se situer entre **5 et 10 minutes** si vous utilisez **Google Colab** avec un GPU.\n",
        "\n",
        "Note : N'oubliez pas d'ajouter le fichier de réponses générées par le modèle (voir *out_file_name*) dans votre remise du travail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "_ntrtd3jWnhs",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-10-19T23:22:25.238643Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ntrtd3jWnhs",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "629eb332-9952-4db7-9de5-2481c0ac3799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q0: Where do Sherlock Holmes and Dr. Watson live?\n",
            "A: Sherlock Holmes and Dr. Watson live in the London suburb of Brixton. They are both doctors\n",
            "Expected:221B Baker Street, London.\n",
            "------------------------------------------------------------\n",
            "Q1: Who is Sherlock Holmes' loyal friend and chronicler?\n",
            "A: Sherlock Holmes' loyal friend and chronicler is the man who has been with him since the beginning\n",
            "Expected:Dr. John H. Watson.\n",
            "------------------------------------------------------------\n",
            "Q2: Who is considered 'The Woman' by Sherlock Holmes?\n",
            "A: The Woman is the most popular woman in the world. She is the most beautiful woman in the world\n",
            "Expected:Irene Adler.\n",
            "------------------------------------------------------------\n",
            "Q3: Which story features the Red-Headed League?\n",
            "A: The Red-Headed League is a fictional organization that is based in the fictional city of New York\n",
            "Expected:The Adventure of the Red-Headed League.\n",
            "------------------------------------------------------------\n",
            "Q4: What is the primary occupation of Sherlock Holmes?\n",
            "A: Sherlock Holmes is a detective who is known for his ability to solve crimes and solve mysteries. He\n",
            "Expected:Consulting detective.\n",
            "------------------------------------------------------------\n",
            "Q5: Who is Sherlock Holmes' arch-nemesis?\n",
            "A: Sherlock Holmes is the most famous detective in the world. He is the most famous detective in the\n",
            "Expected:Professor James Moriarty.\n",
            "------------------------------------------------------------\n",
            "Q6: What musical instrument does Sherlock Holmes play?\n",
            "A: The musical instrument that Sherlock Holmes plays is the violin. He plays the violin in the Sherlock Holmes Adventure\n",
            "Expected:The violin.\n",
            "------------------------------------------------------------\n",
            "Q7: What is the name of Sherlock Holmes' elder brother?\n",
            "A: Sherlock Holmes' elder brother is named Arthur. Arthur is a British scientist and engineer. He is\n",
            "Expected:Mycroft Holmes.\n",
            "------------------------------------------------------------\n",
            "Q8: What is the blue gemstone found inside a Christmas goose called?\n",
            "A: The blue gemstone found inside a Christmas goose called is called a gemstone. It is a rare\n",
            "Expected:The Blue Carbuncle.\n",
            "------------------------------------------------------------\n",
            "Q9: What residue does Holmes often analyze to identify smokers?\n",
            "A: The residue of tobacco is often used to identify smokers. The residue of tobacco is often used to identify\n",
            "Expected:Tobacco ash.\n",
            "------------------------------------------------------------\n",
            "Q10: What tactic besides observation does Holmes often use to gather information?\n",
            "A: Holmes often uses observation to gather information. He often uses observation to gather information about the environment,\n",
            "Expected:Disguise.\n",
            "------------------------------------------------------------\n",
            "Q11: What was the profession of Dr. John Watson?\n",
            "A: Dr. John Watson was a British scientist and philosopher who was the founder of the field of artificial intelligence\n",
            "Expected:Doctor.\n",
            "------------------------------------------------------------\n",
            "Q12: What kind of marks on the ground does Holmes often study to track people?\n",
            "A: The most common type of marks on the ground is footprints. These are small, irregular, and often\n",
            "Expected:Footprints.\n",
            "------------------------------------------------------------\n",
            "Q13: Who is the landlady of 221B Baker Street?\n",
            "A: The landlady of 221B Baker Street is a woman named Mary. She is a retired nurse\n",
            "Expected:Mrs. Hudson.\n",
            "------------------------------------------------------------\n",
            "Q14: What substance did Sherlock Holmes sometimes use to stimulate his mind?\n",
            "A: The substance used to stimulate his mind was a mixture of various plants and herbs. He would often use\n",
            "Expected:Cocaine.\n",
            "------------------------------------------------------------\n",
            "Q15: Who is the Scotland Yard detective that often consults Holmes?\n",
            "A: The Scotland Yard detective that often consults Holmes is Detective Inspector Peter Carey. He is responsible for investigating\n",
            "Expected:Inspector Lestrade (Gregson or Bradstreet also acceptable).\n",
            "------------------------------------------------------------\n",
            "Q16: What warning arrives as envelopes containing dried orange seeds?\n",
            "A: The orange seeds are a common food item in the tropics, and they are often used as a\n",
            "Expected:Five orange pips.\n",
            "------------------------------------------------------------\n",
            "Q17: What is the bog near Baskerville Hall called?\n",
            "A: The bog near Baskerville Hall is a small, rocky area in the centre of the village.\n",
            "Expected:The Grimpen Mire.\n",
            "------------------------------------------------------------\n",
            "Q18: Which London newspaper does Holmes frequently read?\n",
            "A: The London Evening Standard is the most widely read newspaper in the capital. It is also the most popular\n",
            "Expected:The Times.\n",
            "------------------------------------------------------------\n",
            "Q19: What kind of drawings form the code that Holmes deciphers on walls and paper?\n",
            "A: The code that Holmes deciphers on walls and paper is a series of symbols that represent the various\n",
            "Expected:Dancing stick figures.\n",
            "------------------------------------------------------------\n",
            "Q20: What kind of jewel is set in the damaged coronet handled by a banker?\n",
            "A: The jewel is a set of diamonds, set in a damaged coronet. The coronet was handled\n",
            "Expected:Beryls.\n",
            "------------------------------------------------------------\n",
            "Q21: What combat sport is Holmes proficient in?\n",
            "A: Holmes is a professional athlete, and he is a member of the United States Olympic team. He\n",
            "Expected:Boxing.\n",
            "------------------------------------------------------------\n",
            "Q22: What does Sherlock Holmes keep in his Persian slipper?\n",
            "A: The Persian slipper is a slipper that is worn by Persian men. It is a long,\n",
            "Expected:Pipe tobacco.\n",
            "------------------------------------------------------------\n",
            "Q23: In which room at 221B do most client interviews happen?\n",
            "A: The most common interview rooms at 221B are the main office and the conference room. The conference room\n",
            "Expected:The sitting-room.\n",
            "------------------------------------------------------------\n",
            "Q24: What important document goes missing from the Foreign Office in one case?\n",
            "A: The Foreign Office lost a document in one of its cases. The document was a document on the Foreign\n",
            "Expected:A secret naval treaty.\n",
            "------------------------------------------------------------\n",
            "Q25: What hot drink is often served at Baker Street?\n",
            "A: The hot drink is usually served at Baker Street, which is located in the heart of the city.\n",
            "Expected:Tea.\n",
            "------------------------------------------------------------\n",
            "Q26: What object ties Irene Adler to a scandal with a European king?\n",
            "A: The object of the scandal is the death of King Henry VIII, who was accused of having had an\n",
            "Expected:A compromising photograph.\n",
            "------------------------------------------------------------\n",
            "Q27: What weapon does Dr. Watson often carry?\n",
            "A: Dr. Watson often carries a variety of weapons, including a variety of weapons that are not usually seen\n",
            "Expected:A revolver.\n",
            "------------------------------------------------------------\n",
            "Q28: In which country did Dr. Watson serve as an army doctor?\n",
            "A: Dr. Watson served as an army doctor in the United Kingdom from 1883 to 1884. He\n",
            "Expected:Afghanistan.\n",
            "------------------------------------------------------------\n",
            "Q29: What is the missing racehorse’s name in the Dartmoor case?\n",
            "A: The missing racehorse’s name is \"Dartmoor\". The name is a combination\n",
            "Expected:Silver Blaze.\n",
            "------------------------------------------------------------\n",
            "Q30: What London vehicle do Holmes and Watson frequently hire for short trips?\n",
            "A: The London vehicle that Holmes and Watson frequently hire for short trips is a minivan. The minivan\n",
            "Expected:A hansom cab.\n",
            "------------------------------------------------------------\n",
            "Q31: In which English county do Holmes and Watson investigate a deadly household powder?\n",
            "A: In the English county of Berkshire, England, Holmes and Watson investigate a deadly household powder that has been\n",
            "Expected:Cornwall.\n",
            "------------------------------------------------------------\n",
            "Q32: Where does Holmes retire?\n",
            "A: Holmes is retired from the police force. He is currently a consultant for the private sector. He\n",
            "Expected:Sussex Downs.\n",
            "------------------------------------------------------------\n",
            "Q33: What does Watson call Holmes’ method of reasoning?\n",
            "A: The Watson method of reasoning is a method of reasoning that uses the use of deductive reasoning to determine\n",
            "Expected:“The science of deduction and analysis.”\n",
            "------------------------------------------------------------\n",
            "Q34: What is the name of the street‑boy network Holmes employs?\n",
            "A: The name of the street‑boy network is the Holmes Street Network. The network consists of a number\n",
            "Expected:The Baker Street Irregulars.\n",
            "------------------------------------------------------------\n",
            "Q35: What London club is Mycroft Holmes most associated with?\n",
            "A: Mycroft Holmes is most associated with the London club The London Club, which is a professional football club\n",
            "Expected:The Diogenes Club.\n",
            "------------------------------------------------------------\n",
            "Q36: What is the name of the ancient document recited by Reginald Musgrave?\n",
            "A: The ancient document recited by Reginald Musgrave is the Book of the Dead. It is\n",
            "Expected:The Musgrave Ritual.\n",
            "------------------------------------------------------------\n",
            "Q37: What injury does Victor Hatherley suffer during his adventure?\n",
            "A: Victor Hatherley suffers from a severe injury to his right leg. He has been injured in\n",
            "Expected:Loss of a thumb.\n",
            "------------------------------------------------------------\n",
            "Q38: On what moor do the Baskerville events occur?\n",
            "A: The Baskerville events occur on the moor of the Isle of Man. The events are associated\n",
            "Expected:Dartmoor.\n",
            "------------------------------------------------------------\n",
            "Q39: What substance is used to make an animal appear ghostly?\n",
            "A: Ghostly substances are usually used to make animals appear ghostly. They are usually used to make animals\n",
            "Expected:Phosphorus.\n",
            "------------------------------------------------------------\n",
            "Q40: In which county is Dartmoor located?\n",
            "A: Dartmoor is located in the north of England, in the county of Devon. It is\n",
            "Expected:Devonshire.\n",
            "------------------------------------------------------------\n",
            "Q41: What animal do Holmes and Watson sometimes use to track a scent?\n",
            "A: The animal used to track a scent is a dog. Dogs are trained to detect odors and to\n",
            "Expected:A dog.\n",
            "------------------------------------------------------------\n",
            "Q42: What scientific field is Holmes notably skilled in?\n",
            "A: Holmes is a prolific scientist, having published over 1000 scientific papers and numerous books. He is also\n",
            "Expected:Chemistry.\n",
            "------------------------------------------------------------\n",
            "Q43: Which police force does Holmes frequently assist?\n",
            "A: The police force that Holmes frequently assists is the Metropolitan Police Service. The Metropolitan Police Service is a large\n",
            "Expected:Scotland Yard.\n",
            "------------------------------------------------------------\n",
            "Q44: What railway timetable does Holmes often consult?\n",
            "A: Holmes often consults the London Underground timetable, which is usually published in the morning. He also\n",
            "Expected:Bradshaw's.\n",
            "------------------------------------------------------------\n",
            "Q45: What supernatural creature is suspected in a case involving a South American family?\n",
            "A: The family of the missing woman is believed to be the victims of a mysterious illness. The family has\n",
            "Expected:A vampire.\n",
            "------------------------------------------------------------\n",
            "Q46: What quick message service does Holmes often use?\n",
            "A: Holmes uses a variety of messaging services, including instant messaging, instant messaging apps, and social media\n",
            "Expected:Telegrams.\n",
            "------------------------------------------------------------\n",
            "Q47: What handheld tool does Holmes use to inspect tiny clues?\n",
            "A: Holmes uses a small, handheld device called a microscope to examine tiny clues. The device is\n",
            "Expected:A magnifying glass.\n",
            "------------------------------------------------------------\n",
            "Q48: In which city do most of Holmes's cases take place?\n",
            "A: The most common locations for Holmes's cases are London, New York, and Paris. The most famous\n",
            "Expected:London.\n",
            "------------------------------------------------------------\n",
            "Q49: After retirement, what hobby does Holmes pursue?\n",
            "A: Holmes enjoys collecting and collecting things. He has a passion for collecting and collecting things. He has\n",
            "Expected:Beekeeping.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "questions_fn = \"data/questions_sherlock.json\"\n",
        "out_file_name = \"instruct_gpt2_answers.txt\"\n",
        "\n",
        "results = test_on_questions(prompt_builder=alpaca_build_prompt, model_path=instruct_model_path, question_file=questions_fn, out_file_name=out_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df7d2c0",
      "metadata": {
        "id": "4df7d2c0"
      },
      "source": [
        " ## 6. Analyse des résultats\n",
        "\n",
        " ### 6.1 Évaluation quantitative (à compléter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "8d105903",
      "metadata": {
        "id": "8d105903"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "c884b836",
      "metadata": {
        "id": "c884b836"
      },
      "outputs": [],
      "source": [
        "def evaluate_f1(ground_truth, prediction):\n",
        "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en commun et estime précision, rappel et F1.\"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "65fe1607",
      "metadata": {
        "id": "65fe1607"
      },
      "outputs": [],
      "source": [
        "def evaluation_generation(results):\n",
        "    eval = {\"precision\":0, \"recall\":0, \"f1\":0}\n",
        "    for i, question, answer, expected_answer in results:\n",
        "        precision, recall, f1 = evaluate_f1(expected_answer, answer)\n",
        "        eval[\"precision\"] += precision\n",
        "        eval[\"recall\"] += recall\n",
        "        eval[\"f1\"] += f1\n",
        "\n",
        "    eval[\"precision\"] /= len(results)\n",
        "    eval[\"recall\"] /= len(results)\n",
        "    eval[\"f1\"] /= len(results)\n",
        "\n",
        "    return eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "6dd0f919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd0f919",
        "outputId": "245fbb97-1776-458b-bf5e-1c8d64f8d1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.0247940637793579, 'recall': 0.17952380952380953, 'f1': 0.04188327101175398}\n"
          ]
        }
      ],
      "source": [
        "eval = evaluation_generation(results)\n",
        "print(eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521bcbe2",
      "metadata": {
        "id": "521bcbe2"
      },
      "source": [
        "**Question :** Que pensez-vous de cette évaluation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f20fe54",
      "metadata": {
        "id": "8f20fe54"
      },
      "source": [
        "Les résultats sembles s'être fortement dégradé depuis la dernière étape. La précision tombe à 0.02 qui est pire que le résultat pour le premier modèle.\n",
        "\n",
        "Le recall est à 0.179 ce qui est mieu que à la tâche 2 mais pire qu'à la tâche 3.\n",
        "\n",
        "Cela donne un score f1 moyen qui est bien pire qu'avec le premier modèle. Je ne sait pas vraiment comment interpréter ces chiffres plus que cela. Je suis un peu déçu de ce résultat."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d00e33",
      "metadata": {
        "id": "09d00e33"
      },
      "source": [
        "### 6.2 Analyse qualitative (à faire)\n",
        "\n",
        "Faites l'analyse des réponses de ce modèle. Présentez vos observations par rapport aux réponses obtenus des modèles des tâches 2 et 3.\n",
        "\n",
        "Expliquez ce que vous retenez des 3 dernières tâches sur le pré-entraînement et le post-entraînement du modèle GPT-2.\n",
        "\n",
        "Vous pouvez ajouter des cellules au besoin."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En regardant les prédictions, il est possible d'un peu mieux comprendre ce qu'ils se passe.\n",
        "\n",
        "Le fine-tuning avec des intrucctions semble avoir appris au modèle à répondre par des phrases complètes. Ainsi, si les réponses sont plus longues la précision en souffre car elle est calculé en comparant le nombre de mots corrects au nombre de mots dans la prédiction. Le reccal avantagé par les plus longue réponse.\n",
        "\n",
        "Pourtant, l'origine du problème ne m'est pas claire.\n",
        "\n",
        "Le modèle a-t-il été trop entrainé sur les instructions? Cela ferait est sorte qu'il tente de répondre par des réponse élaboré avec des phrases complètes comme dans le jeu d'entrainement.\n",
        "\n",
        "Ou bien le modèle n'a pas été assez entrainé sur le instructions et ainsi, il a de la misère à respecter la consigne de faire un réponse courte.\n",
        "\n",
        "Il serait aussi possible que j'ai trpo entrainé à la tâche 3 et ainsi écrasé des poids nécessaire pour bien comprendre les instructions.\n",
        "\n",
        "Mais ils serait également possible qu'il n'ait pas été suffisament entrainé à l'étape précédente. Cela expliquerait des halucinations comme:\n",
        "\n",
        ">**Q:** What was the profession of Dr. John Watson?\n",
        ">\n",
        ">**A:** Dr. John Watson was a British scientist and philosopher who was the founder of the field of artificial intelligence\n",
        ">\n",
        ">**Expected:** Doctor.\n",
        "\n",
        "Dans tous les cas j'en retien qu'à toutes les étapes d'entrainement, la structure du text d'entrainement est très vite apprise. En effet, lors du pré entrainement, les guillements très présent dans le livre on faire leur apparition dans les prédictions. Également, les prédictions généré après l'entrainnement sur les instruction. on tout aussi vite perdu ces guillements mais ce sont mise à avoir la forme de vrais réponse.\n",
        "\n",
        "Malgré tout l'information semble bien plus difficile à transmettre. Il y a un équilibre difficile entre écraser les poids précédents et capacitées pertinentes à chaque étape.\n",
        "\n"
      ],
      "metadata": {
        "id": "PPEpmMY4_gQd"
      },
      "id": "PPEpmMY4_gQd"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}